{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "53334cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "seed= 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)  # type: ignore\n",
    "torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "torch.backends.cudnn.benchmark = False  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dcc6d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_pickle_path = '/0820_data_inout.pkl' # 경로설정\n",
    "\n",
    "with open(save_pickle_path, 'rb') as infile:\n",
    "\n",
    "    (input_ary, output_ary) = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "13193569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 200)\n",
    "data_col = ['SP_1','SP_2','SP_3','SP_4','SP_56','ANGLE_1','ANGLE_2','ANGLE_3','ANGLE_4','ANGLE_5','ANGLE_6','L1_S1','L1_S2','L2_S1','L2_S2','L3_S1','L3_S2','L4_S1','L4_S2','L5_S1','L5_S2','L6_S1','L6_S2','F1','F2','F3','F4','F5']\n",
    "input_data=pd.DataFrame(input_ary,columns=data_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b0332203",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data=pd.DataFrame(output_ary)\n",
    "yield_data=output_data.iloc[:, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "92e8750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis=input_data.join(yield_data,how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0e699fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis=analysis.rename({64:'yield'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b5807e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = analysis.iloc[:,:-1]\n",
    "y_data = analysis.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b4d8d",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aa8bab13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "epoch 0  | loss: 2.0154  | val_0_mse: 486.66472|  0:00:00s\n",
      "epoch 1  | loss: 1.05677 | val_0_mse: 464.55736|  0:00:00s\n",
      "epoch 2  | loss: 0.71322 | val_0_mse: 255.53901|  0:00:00s\n",
      "epoch 3  | loss: 0.5419  | val_0_mse: 5003.24475|  0:00:00s\n",
      "epoch 4  | loss: 0.344   | val_0_mse: 76424.66973|  0:00:00s\n",
      "epoch 5  | loss: 0.23666 | val_0_mse: 12199.73883|  0:00:00s\n",
      "epoch 6  | loss: 0.24124 | val_0_mse: 5374.94105|  0:00:00s\n",
      "epoch 7  | loss: 0.18979 | val_0_mse: 4220.24603|  0:00:01s\n",
      "epoch 8  | loss: 0.17891 | val_0_mse: 6902.7239|  0:00:01s\n",
      "epoch 9  | loss: 0.15715 | val_0_mse: 12.90049|  0:00:01s\n",
      "epoch 10 | loss: 0.15527 | val_0_mse: 4.04386 |  0:00:01s\n",
      "epoch 11 | loss: 0.13601 | val_0_mse: 2.03119 |  0:00:01s\n",
      "epoch 12 | loss: 0.13058 | val_0_mse: 1.03361 |  0:00:02s\n",
      "epoch 13 | loss: 0.13203 | val_0_mse: 0.60371 |  0:00:02s\n",
      "epoch 14 | loss: 0.11818 | val_0_mse: 1.69007 |  0:00:02s\n",
      "epoch 15 | loss: 0.11716 | val_0_mse: 2.25977 |  0:00:02s\n",
      "epoch 16 | loss: 0.11518 | val_0_mse: 1.60201 |  0:00:02s\n",
      "epoch 17 | loss: 0.10652 | val_0_mse: 0.48839 |  0:00:02s\n",
      "epoch 18 | loss: 0.09876 | val_0_mse: 0.45672 |  0:00:03s\n",
      "epoch 19 | loss: 0.10118 | val_0_mse: 0.55331 |  0:00:03s\n",
      "epoch 20 | loss: 0.08971 | val_0_mse: 0.27184 |  0:00:03s\n",
      "epoch 21 | loss: 0.09324 | val_0_mse: 0.28796 |  0:00:03s\n",
      "epoch 22 | loss: 0.09615 | val_0_mse: 2.14786 |  0:00:03s\n",
      "epoch 23 | loss: 0.09638 | val_0_mse: 3.18304 |  0:00:03s\n",
      "epoch 24 | loss: 0.08995 | val_0_mse: 1.64331 |  0:00:03s\n",
      "epoch 25 | loss: 0.08913 | val_0_mse: 2.8527  |  0:00:03s\n",
      "epoch 26 | loss: 0.09514 | val_0_mse: 3.32527 |  0:00:04s\n",
      "epoch 27 | loss: 0.08115 | val_0_mse: 3.63574 |  0:00:04s\n",
      "epoch 28 | loss: 0.08346 | val_0_mse: 8.24886 |  0:00:04s\n",
      "epoch 29 | loss: 0.0833  | val_0_mse: 5.90073 |  0:00:04s\n",
      "epoch 30 | loss: 0.08397 | val_0_mse: 12.4975 |  0:00:04s\n",
      "epoch 31 | loss: 0.07812 | val_0_mse: 11.15912|  0:00:04s\n",
      "epoch 32 | loss: 0.0819  | val_0_mse: 16.29228|  0:00:04s\n",
      "epoch 33 | loss: 0.08548 | val_0_mse: 15.00762|  0:00:04s\n",
      "epoch 34 | loss: 0.07825 | val_0_mse: 43.43907|  0:00:05s\n",
      "epoch 35 | loss: 0.07923 | val_0_mse: 45.67651|  0:00:05s\n",
      "epoch 36 | loss: 0.08025 | val_0_mse: 41.36062|  0:00:05s\n",
      "epoch 37 | loss: 0.08144 | val_0_mse: 31.94503|  0:00:05s\n",
      "epoch 38 | loss: 0.07728 | val_0_mse: 22.11381|  0:00:05s\n",
      "epoch 39 | loss: 0.07552 | val_0_mse: 13.20272|  0:00:05s\n",
      "epoch 40 | loss: 0.07709 | val_0_mse: 11.08908|  0:00:05s\n",
      "epoch 41 | loss: 0.07561 | val_0_mse: 10.08187|  0:00:05s\n",
      "epoch 42 | loss: 0.07459 | val_0_mse: 9.77453 |  0:00:06s\n",
      "epoch 43 | loss: 0.07695 | val_0_mse: 9.6948  |  0:00:06s\n",
      "epoch 44 | loss: 0.07532 | val_0_mse: 9.6095  |  0:00:06s\n",
      "epoch 45 | loss: 0.07228 | val_0_mse: 9.26983 |  0:00:06s\n",
      "epoch 46 | loss: 0.07365 | val_0_mse: 7.38079 |  0:00:06s\n",
      "epoch 47 | loss: 0.07438 | val_0_mse: 7.06003 |  0:00:06s\n",
      "epoch 48 | loss: 0.07502 | val_0_mse: 4.51706 |  0:00:07s\n",
      "epoch 49 | loss: 0.0764  | val_0_mse: 4.56921 |  0:00:07s\n",
      "epoch 50 | loss: 0.07466 | val_0_mse: 3.81442 |  0:00:07s\n",
      "epoch 51 | loss: 0.07386 | val_0_mse: 3.3603  |  0:00:07s\n",
      "epoch 52 | loss: 0.07403 | val_0_mse: 3.18259 |  0:00:07s\n",
      "epoch 53 | loss: 0.07513 | val_0_mse: 4.07759 |  0:00:07s\n",
      "epoch 54 | loss: 0.07123 | val_0_mse: 4.20842 |  0:00:08s\n",
      "epoch 55 | loss: 0.06924 | val_0_mse: 4.59136 |  0:00:08s\n",
      "epoch 56 | loss: 0.06948 | val_0_mse: 4.45344 |  0:00:08s\n",
      "epoch 57 | loss: 0.07095 | val_0_mse: 5.06799 |  0:00:08s\n",
      "epoch 58 | loss: 0.07411 | val_0_mse: 5.11618 |  0:00:08s\n",
      "epoch 59 | loss: 0.07251 | val_0_mse: 2.73861 |  0:00:08s\n",
      "epoch 60 | loss: 0.07003 | val_0_mse: 1.81002 |  0:00:08s\n",
      "epoch 61 | loss: 0.06688 | val_0_mse: 1.27681 |  0:00:09s\n",
      "epoch 62 | loss: 0.07242 | val_0_mse: 1.21478 |  0:00:09s\n",
      "epoch 63 | loss: 0.0724  | val_0_mse: 1.38811 |  0:00:09s\n",
      "epoch 64 | loss: 0.06805 | val_0_mse: 1.86019 |  0:00:09s\n",
      "epoch 65 | loss: 0.0703  | val_0_mse: 2.30163 |  0:00:09s\n",
      "epoch 66 | loss: 0.07223 | val_0_mse: 2.60147 |  0:00:09s\n",
      "epoch 67 | loss: 0.06541 | val_0_mse: 2.7793  |  0:00:09s\n",
      "epoch 68 | loss: 0.06877 | val_0_mse: 2.90762 |  0:00:09s\n",
      "epoch 69 | loss: 0.06649 | val_0_mse: 3.08139 |  0:00:09s\n",
      "epoch 70 | loss: 0.07051 | val_0_mse: 3.11116 |  0:00:10s\n",
      "epoch 71 | loss: 0.07148 | val_0_mse: 2.82344 |  0:00:10s\n",
      "epoch 72 | loss: 0.07089 | val_0_mse: 2.48013 |  0:00:10s\n",
      "epoch 73 | loss: 0.06505 | val_0_mse: 2.39384 |  0:00:10s\n",
      "epoch 74 | loss: 0.06457 | val_0_mse: 2.28529 |  0:00:10s\n",
      "epoch 75 | loss: 0.07052 | val_0_mse: 2.33143 |  0:00:10s\n",
      "epoch 76 | loss: 0.06888 | val_0_mse: 2.36365 |  0:00:10s\n",
      "epoch 77 | loss: 0.06608 | val_0_mse: 2.4453  |  0:00:10s\n",
      "epoch 78 | loss: 0.06625 | val_0_mse: 2.27551 |  0:00:11s\n",
      "epoch 79 | loss: 0.06979 | val_0_mse: 2.21061 |  0:00:11s\n",
      "epoch 80 | loss: 0.07118 | val_0_mse: 2.22525 |  0:00:11s\n",
      "\n",
      "Early stopping occurred at epoch 80 with best_epoch = 20 and best_val_0_mse = 0.27184\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.74607 | val_0_mse: 242.99765|  0:00:00s\n",
      "epoch 1  | loss: 1.03989 | val_0_mse: 142.41811|  0:00:00s\n",
      "epoch 2  | loss: 0.69902 | val_0_mse: 10095.18747|  0:00:00s\n",
      "epoch 3  | loss: 0.4966  | val_0_mse: 1418.13597|  0:00:00s\n",
      "epoch 4  | loss: 0.33564 | val_0_mse: 903.24436|  0:00:00s\n",
      "epoch 5  | loss: 0.27427 | val_0_mse: 54.72384|  0:00:01s\n",
      "epoch 6  | loss: 0.2679  | val_0_mse: 23.71514|  0:00:01s\n",
      "epoch 7  | loss: 0.20753 | val_0_mse: 44.47146|  0:00:01s\n",
      "epoch 8  | loss: 0.19574 | val_0_mse: 139.60674|  0:00:01s\n",
      "epoch 9  | loss: 0.17437 | val_0_mse: 547.40271|  0:00:01s\n",
      "epoch 10 | loss: 0.16541 | val_0_mse: 493.11661|  0:00:01s\n",
      "epoch 11 | loss: 0.15073 | val_0_mse: 191.64248|  0:00:02s\n",
      "epoch 12 | loss: 0.11859 | val_0_mse: 255.75421|  0:00:02s\n",
      "epoch 13 | loss: 0.12083 | val_0_mse: 168.71443|  0:00:02s\n",
      "epoch 14 | loss: 0.12595 | val_0_mse: 2510.52155|  0:00:02s\n",
      "epoch 15 | loss: 0.12961 | val_0_mse: 256.68816|  0:00:02s\n",
      "epoch 16 | loss: 0.11934 | val_0_mse: 986.3822|  0:00:02s\n",
      "epoch 17 | loss: 0.10555 | val_0_mse: 381.34923|  0:00:02s\n",
      "epoch 18 | loss: 0.1162  | val_0_mse: 343.91473|  0:00:02s\n",
      "epoch 19 | loss: 0.10261 | val_0_mse: 217.26953|  0:00:03s\n",
      "epoch 20 | loss: 0.10874 | val_0_mse: 147.47679|  0:00:03s\n",
      "epoch 21 | loss: 0.09836 | val_0_mse: 122.24899|  0:00:03s\n",
      "epoch 22 | loss: 0.11345 | val_0_mse: 73.35063|  0:00:03s\n",
      "epoch 23 | loss: 0.10318 | val_0_mse: 40.8335 |  0:00:03s\n",
      "epoch 24 | loss: 0.09552 | val_0_mse: 21.39484|  0:00:03s\n",
      "epoch 25 | loss: 0.09002 | val_0_mse: 67.15704|  0:00:03s\n",
      "epoch 26 | loss: 0.09521 | val_0_mse: 10.87064|  0:00:03s\n",
      "epoch 27 | loss: 0.0937  | val_0_mse: 25.60398|  0:00:03s\n",
      "epoch 28 | loss: 0.08861 | val_0_mse: 176.89138|  0:00:04s\n",
      "epoch 29 | loss: 0.0925  | val_0_mse: 151.77891|  0:00:04s\n",
      "epoch 30 | loss: 0.08647 | val_0_mse: 125.00433|  0:00:04s\n",
      "epoch 31 | loss: 0.08621 | val_0_mse: 196.84402|  0:00:04s\n",
      "epoch 32 | loss: 0.08949 | val_0_mse: 189.24419|  0:00:04s\n",
      "epoch 33 | loss: 0.08735 | val_0_mse: 155.86974|  0:00:04s\n",
      "epoch 34 | loss: 0.08429 | val_0_mse: 169.7753|  0:00:04s\n",
      "epoch 35 | loss: 0.086   | val_0_mse: 439.335 |  0:00:04s\n",
      "epoch 36 | loss: 0.08269 | val_0_mse: 424.86164|  0:00:04s\n",
      "epoch 37 | loss: 0.08913 | val_0_mse: 433.94608|  0:00:04s\n",
      "epoch 38 | loss: 0.08324 | val_0_mse: 569.62908|  0:00:05s\n",
      "epoch 39 | loss: 0.08591 | val_0_mse: 472.41033|  0:00:05s\n",
      "epoch 40 | loss: 0.08416 | val_0_mse: 212.04033|  0:00:05s\n",
      "epoch 41 | loss: 0.07619 | val_0_mse: 278.95349|  0:00:05s\n",
      "epoch 42 | loss: 0.0784  | val_0_mse: 207.5977|  0:00:05s\n",
      "epoch 43 | loss: 0.08289 | val_0_mse: 200.85252|  0:00:05s\n",
      "epoch 44 | loss: 0.08086 | val_0_mse: 97.90733|  0:00:05s\n",
      "epoch 45 | loss: 0.07667 | val_0_mse: 90.79444|  0:00:05s\n",
      "epoch 46 | loss: 0.07716 | val_0_mse: 95.00133|  0:00:06s\n",
      "epoch 47 | loss: 0.07563 | val_0_mse: 92.14494|  0:00:06s\n",
      "epoch 48 | loss: 0.07506 | val_0_mse: 93.36254|  0:00:06s\n",
      "epoch 49 | loss: 0.07887 | val_0_mse: 92.27374|  0:00:06s\n",
      "epoch 50 | loss: 0.07019 | val_0_mse: 90.51241|  0:00:06s\n",
      "epoch 51 | loss: 0.07346 | val_0_mse: 91.73807|  0:00:06s\n",
      "epoch 52 | loss: 0.07694 | val_0_mse: 92.29986|  0:00:06s\n",
      "epoch 53 | loss: 0.07637 | val_0_mse: 96.0686 |  0:00:06s\n",
      "epoch 54 | loss: 0.07527 | val_0_mse: 101.11651|  0:00:07s\n",
      "epoch 55 | loss: 0.07615 | val_0_mse: 31.90116|  0:00:07s\n",
      "epoch 56 | loss: 0.07308 | val_0_mse: 3.03383 |  0:00:07s\n",
      "epoch 57 | loss: 0.07341 | val_0_mse: 1.56066 |  0:00:07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58 | loss: 0.07261 | val_0_mse: 56.77709|  0:00:07s\n",
      "epoch 59 | loss: 0.07168 | val_0_mse: 46.44929|  0:00:07s\n",
      "epoch 60 | loss: 0.0733  | val_0_mse: 52.36491|  0:00:07s\n",
      "epoch 61 | loss: 0.07245 | val_0_mse: 76.02832|  0:00:07s\n",
      "epoch 62 | loss: 0.07017 | val_0_mse: 38.36186|  0:00:07s\n",
      "epoch 63 | loss: 0.06741 | val_0_mse: 19.77934|  0:00:08s\n",
      "epoch 64 | loss: 0.07601 | val_0_mse: 28.13618|  0:00:08s\n",
      "epoch 65 | loss: 0.07052 | val_0_mse: 18.25567|  0:00:08s\n",
      "epoch 66 | loss: 0.06909 | val_0_mse: 16.37105|  0:00:08s\n",
      "epoch 67 | loss: 0.07062 | val_0_mse: 15.62784|  0:00:08s\n",
      "epoch 68 | loss: 0.06849 | val_0_mse: 14.14717|  0:00:08s\n",
      "epoch 69 | loss: 0.06776 | val_0_mse: 13.71845|  0:00:09s\n",
      "epoch 70 | loss: 0.06919 | val_0_mse: 12.85721|  0:00:09s\n",
      "epoch 71 | loss: 0.07036 | val_0_mse: 12.6954 |  0:00:09s\n",
      "epoch 72 | loss: 0.06697 | val_0_mse: 13.81908|  0:00:09s\n",
      "epoch 73 | loss: 0.06933 | val_0_mse: 14.59721|  0:00:09s\n",
      "epoch 74 | loss: 0.06932 | val_0_mse: 11.83886|  0:00:09s\n",
      "epoch 75 | loss: 0.06794 | val_0_mse: 7.41225 |  0:00:09s\n",
      "epoch 76 | loss: 0.06766 | val_0_mse: 4.23384 |  0:00:10s\n",
      "epoch 77 | loss: 0.06718 | val_0_mse: 2.71875 |  0:00:10s\n",
      "epoch 78 | loss: 0.06614 | val_0_mse: 2.14542 |  0:00:10s\n",
      "epoch 79 | loss: 0.0686  | val_0_mse: 1.01784 |  0:00:10s\n",
      "epoch 80 | loss: 0.06902 | val_0_mse: 7.66881 |  0:00:10s\n",
      "epoch 81 | loss: 0.07008 | val_0_mse: 5.63498 |  0:00:10s\n",
      "epoch 82 | loss: 0.06657 | val_0_mse: 11.43911|  0:00:10s\n",
      "epoch 83 | loss: 0.06723 | val_0_mse: 10.37796|  0:00:11s\n",
      "epoch 84 | loss: 0.06735 | val_0_mse: 10.26859|  0:00:11s\n",
      "epoch 85 | loss: 0.06636 | val_0_mse: 5.57195 |  0:00:11s\n",
      "epoch 86 | loss: 0.06377 | val_0_mse: 3.10718 |  0:00:11s\n",
      "epoch 87 | loss: 0.06677 | val_0_mse: 3.4815  |  0:00:11s\n",
      "epoch 88 | loss: 0.06715 | val_0_mse: 5.98558 |  0:00:11s\n",
      "epoch 89 | loss: 0.0633  | val_0_mse: 6.54756 |  0:00:11s\n",
      "epoch 90 | loss: 0.06453 | val_0_mse: 7.28814 |  0:00:12s\n",
      "epoch 91 | loss: 0.06555 | val_0_mse: 8.17943 |  0:00:12s\n",
      "epoch 92 | loss: 0.06643 | val_0_mse: 7.2304  |  0:00:12s\n",
      "epoch 93 | loss: 0.06733 | val_0_mse: 5.51136 |  0:00:12s\n",
      "epoch 94 | loss: 0.06261 | val_0_mse: 1.62234 |  0:00:12s\n",
      "epoch 95 | loss: 0.06391 | val_0_mse: 1.74246 |  0:00:12s\n",
      "epoch 96 | loss: 0.06496 | val_0_mse: 2.70268 |  0:00:13s\n",
      "epoch 97 | loss: 0.06558 | val_0_mse: 5.56583 |  0:00:13s\n",
      "epoch 98 | loss: 0.06706 | val_0_mse: 5.98422 |  0:00:13s\n",
      "epoch 99 | loss: 0.06377 | val_0_mse: 5.35172 |  0:00:13s\n",
      "epoch 100| loss: 0.06455 | val_0_mse: 4.2977  |  0:00:13s\n",
      "epoch 101| loss: 0.0666  | val_0_mse: 2.14285 |  0:00:13s\n",
      "epoch 102| loss: 0.06486 | val_0_mse: 1.54224 |  0:00:13s\n",
      "epoch 103| loss: 0.06403 | val_0_mse: 3.90689 |  0:00:14s\n",
      "epoch 104| loss: 0.06264 | val_0_mse: 1.10597 |  0:00:14s\n",
      "epoch 105| loss: 0.06278 | val_0_mse: 1.65192 |  0:00:14s\n",
      "epoch 106| loss: 0.06499 | val_0_mse: 3.05382 |  0:00:14s\n",
      "epoch 107| loss: 0.06598 | val_0_mse: 3.37294 |  0:00:14s\n",
      "epoch 108| loss: 0.06443 | val_0_mse: 0.44281 |  0:00:14s\n",
      "epoch 109| loss: 0.06477 | val_0_mse: 0.43831 |  0:00:14s\n",
      "epoch 110| loss: 0.06498 | val_0_mse: 0.51261 |  0:00:14s\n",
      "epoch 111| loss: 0.0618  | val_0_mse: 0.74378 |  0:00:14s\n",
      "epoch 112| loss: 0.0609  | val_0_mse: 0.64609 |  0:00:15s\n",
      "epoch 113| loss: 0.06236 | val_0_mse: 0.98032 |  0:00:15s\n",
      "epoch 114| loss: 0.06623 | val_0_mse: 1.02778 |  0:00:15s\n",
      "epoch 115| loss: 0.06375 | val_0_mse: 0.88604 |  0:00:15s\n",
      "epoch 116| loss: 0.06111 | val_0_mse: 0.9462  |  0:00:15s\n",
      "epoch 117| loss: 0.06598 | val_0_mse: 0.72323 |  0:00:15s\n",
      "epoch 118| loss: 0.05946 | val_0_mse: 0.78135 |  0:00:15s\n",
      "epoch 119| loss: 0.06544 | val_0_mse: 0.61162 |  0:00:15s\n",
      "epoch 120| loss: 0.06183 | val_0_mse: 0.47077 |  0:00:16s\n",
      "epoch 121| loss: 0.06312 | val_0_mse: 0.5978  |  0:00:16s\n",
      "epoch 122| loss: 0.06157 | val_0_mse: 0.56108 |  0:00:16s\n",
      "epoch 123| loss: 0.06362 | val_0_mse: 0.58435 |  0:00:16s\n",
      "epoch 124| loss: 0.06083 | val_0_mse: 0.31889 |  0:00:16s\n",
      "epoch 125| loss: 0.06331 | val_0_mse: 0.20428 |  0:00:16s\n",
      "epoch 126| loss: 0.06262 | val_0_mse: 0.23801 |  0:00:16s\n",
      "epoch 127| loss: 0.06177 | val_0_mse: 0.24163 |  0:00:16s\n",
      "epoch 128| loss: 0.05874 | val_0_mse: 0.27269 |  0:00:17s\n",
      "epoch 129| loss: 0.06538 | val_0_mse: 0.17774 |  0:00:17s\n",
      "epoch 130| loss: 0.06395 | val_0_mse: 0.42159 |  0:00:17s\n",
      "epoch 131| loss: 0.06006 | val_0_mse: 0.40053 |  0:00:17s\n",
      "epoch 132| loss: 0.06021 | val_0_mse: 0.65982 |  0:00:17s\n",
      "epoch 133| loss: 0.06117 | val_0_mse: 0.71737 |  0:00:17s\n",
      "epoch 134| loss: 0.06523 | val_0_mse: 0.84162 |  0:00:17s\n",
      "epoch 135| loss: 0.05749 | val_0_mse: 1.19183 |  0:00:17s\n",
      "epoch 136| loss: 0.06301 | val_0_mse: 1.21458 |  0:00:18s\n",
      "epoch 137| loss: 0.06196 | val_0_mse: 1.19838 |  0:00:18s\n",
      "epoch 138| loss: 0.06266 | val_0_mse: 0.95425 |  0:00:18s\n",
      "epoch 139| loss: 0.06134 | val_0_mse: 1.02623 |  0:00:18s\n",
      "epoch 140| loss: 0.06281 | val_0_mse: 1.27463 |  0:00:18s\n",
      "epoch 141| loss: 0.06178 | val_0_mse: 0.85893 |  0:00:18s\n",
      "epoch 142| loss: 0.06101 | val_0_mse: 0.65728 |  0:00:18s\n",
      "epoch 143| loss: 0.0637  | val_0_mse: 0.48755 |  0:00:18s\n",
      "epoch 144| loss: 0.06071 | val_0_mse: 0.43867 |  0:00:18s\n",
      "epoch 145| loss: 0.0593  | val_0_mse: 1.03599 |  0:00:19s\n",
      "epoch 146| loss: 0.06156 | val_0_mse: 0.7125  |  0:00:19s\n",
      "epoch 147| loss: 0.06176 | val_0_mse: 0.81406 |  0:00:19s\n",
      "epoch 148| loss: 0.06009 | val_0_mse: 1.03663 |  0:00:19s\n",
      "epoch 149| loss: 0.05783 | val_0_mse: 0.81512 |  0:00:19s\n",
      "epoch 150| loss: 0.06106 | val_0_mse: 0.71355 |  0:00:19s\n",
      "epoch 151| loss: 0.05886 | val_0_mse: 0.72156 |  0:00:19s\n",
      "epoch 152| loss: 0.06115 | val_0_mse: 0.72135 |  0:00:19s\n",
      "epoch 153| loss: 0.06059 | val_0_mse: 0.74058 |  0:00:19s\n",
      "epoch 154| loss: 0.06261 | val_0_mse: 0.73814 |  0:00:20s\n",
      "epoch 155| loss: 0.05803 | val_0_mse: 0.64502 |  0:00:20s\n",
      "epoch 156| loss: 0.06143 | val_0_mse: 0.585   |  0:00:20s\n",
      "epoch 157| loss: 0.05979 | val_0_mse: 0.49585 |  0:00:20s\n",
      "epoch 158| loss: 0.05879 | val_0_mse: 0.35128 |  0:00:20s\n",
      "epoch 159| loss: 0.06032 | val_0_mse: 0.37692 |  0:00:20s\n",
      "epoch 160| loss: 0.06053 | val_0_mse: 0.49093 |  0:00:20s\n",
      "epoch 161| loss: 0.06335 | val_0_mse: 0.43845 |  0:00:20s\n",
      "epoch 162| loss: 0.06084 | val_0_mse: 0.3379  |  0:00:20s\n",
      "epoch 163| loss: 0.05989 | val_0_mse: 0.272   |  0:00:21s\n",
      "epoch 164| loss: 0.0588  | val_0_mse: 0.30228 |  0:00:21s\n",
      "epoch 165| loss: 0.05959 | val_0_mse: 0.34868 |  0:00:21s\n",
      "epoch 166| loss: 0.05969 | val_0_mse: 0.38413 |  0:00:21s\n",
      "epoch 167| loss: 0.06099 | val_0_mse: 0.39289 |  0:00:21s\n",
      "epoch 168| loss: 0.06251 | val_0_mse: 0.42058 |  0:00:21s\n",
      "epoch 169| loss: 0.06141 | val_0_mse: 0.27274 |  0:00:21s\n",
      "epoch 170| loss: 0.05889 | val_0_mse: 0.21569 |  0:00:21s\n",
      "epoch 171| loss: 0.06138 | val_0_mse: 0.15986 |  0:00:22s\n",
      "epoch 172| loss: 0.06228 | val_0_mse: 0.17446 |  0:00:22s\n",
      "epoch 173| loss: 0.05981 | val_0_mse: 0.24464 |  0:00:22s\n",
      "epoch 174| loss: 0.05899 | val_0_mse: 0.39244 |  0:00:22s\n",
      "epoch 175| loss: 0.06219 | val_0_mse: 0.57591 |  0:00:22s\n",
      "epoch 176| loss: 0.0589  | val_0_mse: 0.72656 |  0:00:22s\n",
      "epoch 177| loss: 0.06284 | val_0_mse: 0.8145  |  0:00:22s\n",
      "epoch 178| loss: 0.05989 | val_0_mse: 0.83123 |  0:00:23s\n",
      "epoch 179| loss: 0.06054 | val_0_mse: 0.85166 |  0:00:23s\n",
      "epoch 180| loss: 0.05851 | val_0_mse: 0.82942 |  0:00:23s\n",
      "epoch 181| loss: 0.06224 | val_0_mse: 1.02886 |  0:00:23s\n",
      "epoch 182| loss: 0.05632 | val_0_mse: 1.99184 |  0:00:23s\n",
      "epoch 183| loss: 0.06034 | val_0_mse: 1.17339 |  0:00:23s\n",
      "epoch 184| loss: 0.06064 | val_0_mse: 0.99646 |  0:00:23s\n",
      "epoch 185| loss: 0.05956 | val_0_mse: 0.70231 |  0:00:24s\n",
      "epoch 186| loss: 0.06153 | val_0_mse: 0.65724 |  0:00:24s\n",
      "epoch 187| loss: 0.05881 | val_0_mse: 0.59613 |  0:00:24s\n",
      "epoch 188| loss: 0.05911 | val_0_mse: 0.54133 |  0:00:24s\n",
      "epoch 189| loss: 0.05895 | val_0_mse: 0.45455 |  0:00:24s\n",
      "epoch 190| loss: 0.06071 | val_0_mse: 0.71143 |  0:00:24s\n",
      "epoch 191| loss: 0.05987 | val_0_mse: 0.36387 |  0:00:24s\n",
      "epoch 192| loss: 0.06265 | val_0_mse: 0.27411 |  0:00:24s\n",
      "epoch 193| loss: 0.06031 | val_0_mse: 0.23241 |  0:00:25s\n",
      "epoch 194| loss: 0.05782 | val_0_mse: 0.22993 |  0:00:25s\n",
      "epoch 195| loss: 0.06077 | val_0_mse: 0.25601 |  0:00:25s\n",
      "epoch 196| loss: 0.05787 | val_0_mse: 0.24179 |  0:00:25s\n",
      "epoch 197| loss: 0.05871 | val_0_mse: 0.22697 |  0:00:25s\n",
      "epoch 198| loss: 0.05942 | val_0_mse: 0.20859 |  0:00:25s\n",
      "epoch 199| loss: 0.05847 | val_0_mse: 0.19797 |  0:00:25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200| loss: 0.05809 | val_0_mse: 0.2046  |  0:00:26s\n",
      "epoch 201| loss: 0.05783 | val_0_mse: 0.23084 |  0:00:26s\n",
      "epoch 202| loss: 0.05812 | val_0_mse: 0.24199 |  0:00:26s\n",
      "epoch 203| loss: 0.0587  | val_0_mse: 0.25666 |  0:00:26s\n",
      "epoch 204| loss: 0.0547  | val_0_mse: 0.24499 |  0:00:26s\n",
      "epoch 205| loss: 0.05951 | val_0_mse: 0.25905 |  0:00:26s\n",
      "epoch 206| loss: 0.05698 | val_0_mse: 0.26341 |  0:00:26s\n",
      "epoch 207| loss: 0.05811 | val_0_mse: 0.3007  |  0:00:27s\n",
      "epoch 208| loss: 0.05877 | val_0_mse: 0.38078 |  0:00:27s\n",
      "epoch 209| loss: 0.05642 | val_0_mse: 0.3195  |  0:00:27s\n",
      "epoch 210| loss: 0.05628 | val_0_mse: 0.28595 |  0:00:27s\n",
      "epoch 211| loss: 0.0566  | val_0_mse: 0.25638 |  0:00:27s\n",
      "epoch 212| loss: 0.05797 | val_0_mse: 0.23188 |  0:00:27s\n",
      "epoch 213| loss: 0.05778 | val_0_mse: 0.22643 |  0:00:27s\n",
      "epoch 214| loss: 0.05868 | val_0_mse: 0.25246 |  0:00:27s\n",
      "epoch 215| loss: 0.05774 | val_0_mse: 0.24943 |  0:00:27s\n",
      "epoch 216| loss: 0.05494 | val_0_mse: 0.29642 |  0:00:28s\n",
      "epoch 217| loss: 0.05502 | val_0_mse: 0.25973 |  0:00:28s\n",
      "epoch 218| loss: 0.05643 | val_0_mse: 0.18991 |  0:00:28s\n",
      "epoch 219| loss: 0.05405 | val_0_mse: 0.23716 |  0:00:28s\n",
      "epoch 220| loss: 0.05641 | val_0_mse: 0.25693 |  0:00:28s\n",
      "epoch 221| loss: 0.05494 | val_0_mse: 0.31098 |  0:00:28s\n",
      "epoch 222| loss: 0.05489 | val_0_mse: 0.4127  |  0:00:28s\n",
      "epoch 223| loss: 0.05757 | val_0_mse: 0.37741 |  0:00:28s\n",
      "epoch 224| loss: 0.0555  | val_0_mse: 0.36669 |  0:00:29s\n",
      "epoch 225| loss: 0.05659 | val_0_mse: 0.41117 |  0:00:29s\n",
      "epoch 226| loss: 0.05771 | val_0_mse: 0.57977 |  0:00:29s\n",
      "epoch 227| loss: 0.05693 | val_0_mse: 0.69653 |  0:00:29s\n",
      "epoch 228| loss: 0.05527 | val_0_mse: 0.81634 |  0:00:29s\n",
      "epoch 229| loss: 0.06109 | val_0_mse: 1.03247 |  0:00:29s\n",
      "epoch 230| loss: 0.05967 | val_0_mse: 0.92267 |  0:00:29s\n",
      "epoch 231| loss: 0.0585  | val_0_mse: 0.90796 |  0:00:30s\n",
      "\n",
      "Early stopping occurred at epoch 231 with best_epoch = 171 and best_val_0_mse = 0.15986\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.42941 | val_0_mse: 967.89629|  0:00:00s\n",
      "epoch 1  | loss: 0.87748 | val_0_mse: 74.82963|  0:00:00s\n",
      "epoch 2  | loss: 0.65837 | val_0_mse: 29768.32067|  0:00:00s\n",
      "epoch 3  | loss: 0.42988 | val_0_mse: 530.54339|  0:00:00s\n",
      "epoch 4  | loss: 0.39696 | val_0_mse: 103.87985|  0:00:00s\n",
      "epoch 5  | loss: 0.34166 | val_0_mse: 192.72464|  0:00:00s\n",
      "epoch 6  | loss: 0.27814 | val_0_mse: 6478.66322|  0:00:00s\n",
      "epoch 7  | loss: 0.18909 | val_0_mse: 1439.72575|  0:00:01s\n",
      "epoch 8  | loss: 0.18233 | val_0_mse: 5309.67462|  0:00:01s\n",
      "epoch 9  | loss: 0.17504 | val_0_mse: 8350.45576|  0:00:01s\n",
      "epoch 10 | loss: 0.16644 | val_0_mse: 19822.41214|  0:00:01s\n",
      "epoch 11 | loss: 0.22953 | val_0_mse: 12676.46825|  0:00:01s\n",
      "epoch 12 | loss: 0.13868 | val_0_mse: 6074.04835|  0:00:01s\n",
      "epoch 13 | loss: 0.1331  | val_0_mse: 2907.72197|  0:00:02s\n",
      "epoch 14 | loss: 0.12694 | val_0_mse: 1297.69553|  0:00:02s\n",
      "epoch 15 | loss: 0.12826 | val_0_mse: 139.02199|  0:00:02s\n",
      "epoch 16 | loss: 0.11828 | val_0_mse: 36.51523|  0:00:02s\n",
      "epoch 17 | loss: 0.10112 | val_0_mse: 25.85599|  0:00:02s\n",
      "epoch 18 | loss: 0.10173 | val_0_mse: 81.85306|  0:00:02s\n",
      "epoch 19 | loss: 0.10354 | val_0_mse: 161.47423|  0:00:02s\n",
      "epoch 20 | loss: 0.10642 | val_0_mse: 931.44873|  0:00:02s\n",
      "epoch 21 | loss: 0.1007  | val_0_mse: 786.34903|  0:00:03s\n",
      "epoch 22 | loss: 0.10055 | val_0_mse: 897.27227|  0:00:03s\n",
      "epoch 23 | loss: 0.09543 | val_0_mse: 131.33143|  0:00:03s\n",
      "epoch 24 | loss: 0.09571 | val_0_mse: 213.93403|  0:00:03s\n",
      "epoch 25 | loss: 0.08795 | val_0_mse: 440.97626|  0:00:03s\n",
      "epoch 26 | loss: 0.09373 | val_0_mse: 560.95359|  0:00:03s\n",
      "epoch 27 | loss: 0.08272 | val_0_mse: 511.93765|  0:00:03s\n",
      "epoch 28 | loss: 0.08751 | val_0_mse: 405.88016|  0:00:03s\n",
      "epoch 29 | loss: 0.08087 | val_0_mse: 237.96053|  0:00:04s\n",
      "epoch 30 | loss: 0.08342 | val_0_mse: 114.328 |  0:00:04s\n",
      "epoch 31 | loss: 0.08616 | val_0_mse: 64.7278 |  0:00:04s\n",
      "epoch 32 | loss: 0.08262 | val_0_mse: 33.55343|  0:00:04s\n",
      "epoch 33 | loss: 0.07487 | val_0_mse: 12.08886|  0:00:04s\n",
      "epoch 34 | loss: 0.07621 | val_0_mse: 5.56255 |  0:00:04s\n",
      "epoch 35 | loss: 0.07482 | val_0_mse: 1.51954 |  0:00:04s\n",
      "epoch 36 | loss: 0.07939 | val_0_mse: 1.48005 |  0:00:04s\n",
      "epoch 37 | loss: 0.07793 | val_0_mse: 1.17166 |  0:00:05s\n",
      "epoch 38 | loss: 0.07992 | val_0_mse: 1.00792 |  0:00:05s\n",
      "epoch 39 | loss: 0.07561 | val_0_mse: 0.91233 |  0:00:05s\n",
      "epoch 40 | loss: 0.07545 | val_0_mse: 0.88972 |  0:00:05s\n",
      "epoch 41 | loss: 0.07682 | val_0_mse: 1.02375 |  0:00:05s\n",
      "epoch 42 | loss: 0.06914 | val_0_mse: 0.77954 |  0:00:05s\n",
      "epoch 43 | loss: 0.0727  | val_0_mse: 0.84637 |  0:00:05s\n",
      "epoch 44 | loss: 0.07263 | val_0_mse: 1.02785 |  0:00:05s\n",
      "epoch 45 | loss: 0.06916 | val_0_mse: 0.82371 |  0:00:06s\n",
      "epoch 46 | loss: 0.08711 | val_0_mse: 0.9128  |  0:00:06s\n",
      "epoch 47 | loss: 0.06622 | val_0_mse: 0.92598 |  0:00:06s\n",
      "epoch 48 | loss: 0.07333 | val_0_mse: 1.91577 |  0:00:06s\n",
      "epoch 49 | loss: 0.07194 | val_0_mse: 0.85748 |  0:00:06s\n",
      "epoch 50 | loss: 0.07268 | val_0_mse: 0.77003 |  0:00:07s\n",
      "epoch 51 | loss: 0.06888 | val_0_mse: 0.91437 |  0:00:07s\n",
      "epoch 52 | loss: 0.0707  | val_0_mse: 0.96104 |  0:00:07s\n",
      "epoch 53 | loss: 0.07173 | val_0_mse: 0.79478 |  0:00:07s\n",
      "epoch 54 | loss: 0.06823 | val_0_mse: 0.4003  |  0:00:07s\n",
      "epoch 55 | loss: 0.07025 | val_0_mse: 2.07433 |  0:00:08s\n",
      "epoch 56 | loss: 0.06779 | val_0_mse: 13.72191|  0:00:08s\n",
      "epoch 57 | loss: 0.06633 | val_0_mse: 49.56627|  0:00:08s\n",
      "epoch 58 | loss: 0.06476 | val_0_mse: 88.01837|  0:00:08s\n",
      "epoch 59 | loss: 0.0681  | val_0_mse: 161.3701|  0:00:08s\n",
      "epoch 60 | loss: 0.07111 | val_0_mse: 210.78092|  0:00:08s\n",
      "epoch 61 | loss: 0.06674 | val_0_mse: 198.73005|  0:00:08s\n",
      "epoch 62 | loss: 0.07247 | val_0_mse: 135.85499|  0:00:09s\n",
      "epoch 63 | loss: 0.0655  | val_0_mse: 118.24722|  0:00:09s\n",
      "epoch 64 | loss: 0.06623 | val_0_mse: 97.17356|  0:00:09s\n",
      "epoch 65 | loss: 0.06418 | val_0_mse: 53.49145|  0:00:09s\n",
      "epoch 66 | loss: 0.06519 | val_0_mse: 32.29196|  0:00:09s\n",
      "epoch 67 | loss: 0.06614 | val_0_mse: 15.61821|  0:00:09s\n",
      "epoch 68 | loss: 0.06543 | val_0_mse: 7.61531 |  0:00:09s\n",
      "epoch 69 | loss: 0.06309 | val_0_mse: 0.18265 |  0:00:09s\n",
      "epoch 70 | loss: 0.06301 | val_0_mse: 0.12727 |  0:00:09s\n",
      "epoch 71 | loss: 0.07086 | val_0_mse: 0.12076 |  0:00:10s\n",
      "epoch 72 | loss: 0.06495 | val_0_mse: 0.12114 |  0:00:10s\n",
      "epoch 73 | loss: 0.06827 | val_0_mse: 0.12862 |  0:00:10s\n",
      "epoch 74 | loss: 0.06345 | val_0_mse: 0.13045 |  0:00:10s\n",
      "epoch 75 | loss: 0.06411 | val_0_mse: 0.1317  |  0:00:10s\n",
      "epoch 76 | loss: 0.06059 | val_0_mse: 0.13527 |  0:00:10s\n",
      "epoch 77 | loss: 0.06444 | val_0_mse: 0.14166 |  0:00:10s\n",
      "epoch 78 | loss: 0.06326 | val_0_mse: 0.14452 |  0:00:10s\n",
      "epoch 79 | loss: 0.06131 | val_0_mse: 0.14718 |  0:00:11s\n",
      "epoch 80 | loss: 0.0619  | val_0_mse: 0.14294 |  0:00:11s\n",
      "epoch 81 | loss: 0.06092 | val_0_mse: 0.14233 |  0:00:11s\n",
      "epoch 82 | loss: 0.05978 | val_0_mse: 0.13979 |  0:00:11s\n",
      "epoch 83 | loss: 0.06198 | val_0_mse: 0.12408 |  0:00:11s\n",
      "epoch 84 | loss: 0.06425 | val_0_mse: 16.92568|  0:00:12s\n",
      "epoch 85 | loss: 0.06087 | val_0_mse: 0.13552 |  0:00:12s\n",
      "epoch 86 | loss: 0.06104 | val_0_mse: 0.14743 |  0:00:12s\n",
      "epoch 87 | loss: 0.05996 | val_0_mse: 0.15694 |  0:00:12s\n",
      "epoch 88 | loss: 0.06507 | val_0_mse: 0.16682 |  0:00:12s\n",
      "epoch 89 | loss: 0.06156 | val_0_mse: 0.17559 |  0:00:12s\n",
      "epoch 90 | loss: 0.06033 | val_0_mse: 0.19334 |  0:00:12s\n",
      "epoch 91 | loss: 0.05994 | val_0_mse: 0.19406 |  0:00:13s\n",
      "epoch 92 | loss: 0.06008 | val_0_mse: 3.15448 |  0:00:13s\n",
      "epoch 93 | loss: 0.06116 | val_0_mse: 0.30044 |  0:00:13s\n",
      "epoch 94 | loss: 0.0572  | val_0_mse: 0.28828 |  0:00:13s\n",
      "epoch 95 | loss: 0.05937 | val_0_mse: 0.24103 |  0:00:13s\n",
      "epoch 96 | loss: 0.05909 | val_0_mse: 7.3207  |  0:00:13s\n",
      "epoch 97 | loss: 0.06079 | val_0_mse: 0.20964 |  0:00:13s\n",
      "epoch 98 | loss: 0.06035 | val_0_mse: 0.18089 |  0:00:13s\n",
      "epoch 99 | loss: 0.06026 | val_0_mse: 0.17045 |  0:00:14s\n",
      "epoch 100| loss: 0.05941 | val_0_mse: 0.20508 |  0:00:14s\n",
      "epoch 101| loss: 0.06253 | val_0_mse: 0.15084 |  0:00:14s\n",
      "epoch 102| loss: 0.06205 | val_0_mse: 0.1478  |  0:00:14s\n",
      "epoch 103| loss: 0.05917 | val_0_mse: 0.1435  |  0:00:14s\n",
      "epoch 104| loss: 0.06339 | val_0_mse: 0.14311 |  0:00:14s\n",
      "epoch 105| loss: 0.05943 | val_0_mse: 0.14523 |  0:00:14s\n",
      "epoch 106| loss: 0.05778 | val_0_mse: 0.15333 |  0:00:14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 107| loss: 0.06371 | val_0_mse: 0.15057 |  0:00:15s\n",
      "epoch 108| loss: 0.05683 | val_0_mse: 0.14809 |  0:00:15s\n",
      "epoch 109| loss: 0.06334 | val_0_mse: 0.15185 |  0:00:15s\n",
      "epoch 110| loss: 0.05885 | val_0_mse: 0.15169 |  0:00:15s\n",
      "epoch 111| loss: 0.06075 | val_0_mse: 0.15691 |  0:00:15s\n",
      "epoch 112| loss: 0.05694 | val_0_mse: 0.16314 |  0:00:15s\n",
      "epoch 113| loss: 0.05688 | val_0_mse: 0.17141 |  0:00:15s\n",
      "epoch 114| loss: 0.05845 | val_0_mse: 0.17693 |  0:00:15s\n",
      "epoch 115| loss: 0.05846 | val_0_mse: 0.19517 |  0:00:16s\n",
      "epoch 116| loss: 0.05827 | val_0_mse: 0.17781 |  0:00:16s\n",
      "epoch 117| loss: 0.06266 | val_0_mse: 0.17706 |  0:00:16s\n",
      "epoch 118| loss: 0.05723 | val_0_mse: 0.17086 |  0:00:16s\n",
      "epoch 119| loss: 0.05657 | val_0_mse: 0.1573  |  0:00:16s\n",
      "epoch 120| loss: 0.05911 | val_0_mse: 0.15321 |  0:00:16s\n",
      "epoch 121| loss: 0.05625 | val_0_mse: 0.1604  |  0:00:16s\n",
      "epoch 122| loss: 0.05625 | val_0_mse: 0.17267 |  0:00:17s\n",
      "epoch 123| loss: 0.05655 | val_0_mse: 0.19017 |  0:00:17s\n",
      "epoch 124| loss: 0.05941 | val_0_mse: 0.18632 |  0:00:17s\n",
      "epoch 125| loss: 0.05822 | val_0_mse: 0.18647 |  0:00:17s\n",
      "epoch 126| loss: 0.05644 | val_0_mse: 0.18824 |  0:00:17s\n",
      "epoch 127| loss: 0.05594 | val_0_mse: 0.19272 |  0:00:17s\n",
      "epoch 128| loss: 0.05798 | val_0_mse: 0.19079 |  0:00:17s\n",
      "epoch 129| loss: 0.05739 | val_0_mse: 0.20369 |  0:00:18s\n",
      "epoch 130| loss: 0.0574  | val_0_mse: 0.18701 |  0:00:18s\n",
      "epoch 131| loss: 0.05686 | val_0_mse: 0.17067 |  0:00:18s\n",
      "\n",
      "Early stopping occurred at epoch 131 with best_epoch = 71 and best_val_0_mse = 0.12076\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.75512 | val_0_mse: 96.23201|  0:00:00s\n",
      "epoch 1  | loss: 1.28132 | val_0_mse: 16.5496 |  0:00:00s\n",
      "epoch 2  | loss: 0.75098 | val_0_mse: 24.0711 |  0:00:00s\n",
      "epoch 3  | loss: 0.55828 | val_0_mse: 32.46693|  0:00:00s\n",
      "epoch 4  | loss: 0.41819 | val_0_mse: 25.3722 |  0:00:00s\n",
      "epoch 5  | loss: 0.31286 | val_0_mse: 118.12  |  0:00:00s\n",
      "epoch 6  | loss: 0.21543 | val_0_mse: 821.51107|  0:00:01s\n",
      "epoch 7  | loss: 0.24544 | val_0_mse: 1301.2347|  0:00:01s\n",
      "epoch 8  | loss: 0.23741 | val_0_mse: 3610.74389|  0:00:01s\n",
      "epoch 9  | loss: 0.17208 | val_0_mse: 5868.04245|  0:00:01s\n",
      "epoch 10 | loss: 0.15443 | val_0_mse: 5113.20949|  0:00:01s\n",
      "epoch 11 | loss: 0.11353 | val_0_mse: 5733.81299|  0:00:01s\n",
      "epoch 12 | loss: 0.11544 | val_0_mse: 4102.95197|  0:00:01s\n",
      "epoch 13 | loss: 0.09918 | val_0_mse: 2924.55672|  0:00:02s\n",
      "epoch 14 | loss: 0.10952 | val_0_mse: 1260.72162|  0:00:02s\n",
      "epoch 15 | loss: 0.09106 | val_0_mse: 81.42984|  0:00:02s\n",
      "epoch 16 | loss: 0.08645 | val_0_mse: 0.74316 |  0:00:02s\n",
      "epoch 17 | loss: 0.08615 | val_0_mse: 0.51761 |  0:00:02s\n",
      "epoch 18 | loss: 0.08224 | val_0_mse: 0.72422 |  0:00:02s\n",
      "epoch 19 | loss: 0.08079 | val_0_mse: 0.42603 |  0:00:02s\n",
      "epoch 20 | loss: 0.08209 | val_0_mse: 0.27713 |  0:00:02s\n",
      "epoch 21 | loss: 0.0814  | val_0_mse: 0.29441 |  0:00:03s\n",
      "epoch 22 | loss: 0.0881  | val_0_mse: 0.3575  |  0:00:03s\n",
      "epoch 23 | loss: 0.07981 | val_0_mse: 0.4393  |  0:00:03s\n",
      "epoch 24 | loss: 0.08163 | val_0_mse: 0.50214 |  0:00:03s\n",
      "epoch 25 | loss: 0.07911 | val_0_mse: 0.63169 |  0:00:03s\n",
      "epoch 26 | loss: 0.07921 | val_0_mse: 0.86053 |  0:00:03s\n",
      "epoch 27 | loss: 0.07133 | val_0_mse: 1.08612 |  0:00:03s\n",
      "epoch 28 | loss: 0.07643 | val_0_mse: 1.34521 |  0:00:03s\n",
      "epoch 29 | loss: 0.07462 | val_0_mse: 1.64311 |  0:00:03s\n",
      "epoch 30 | loss: 0.07569 | val_0_mse: 1.82756 |  0:00:04s\n",
      "epoch 31 | loss: 0.07706 | val_0_mse: 1.86781 |  0:00:04s\n",
      "epoch 32 | loss: 0.07073 | val_0_mse: 1.9083  |  0:00:04s\n",
      "epoch 33 | loss: 0.07093 | val_0_mse: 2.03138 |  0:00:04s\n",
      "epoch 34 | loss: 0.07313 | val_0_mse: 2.1139  |  0:00:04s\n",
      "epoch 35 | loss: 0.0715  | val_0_mse: 2.27674 |  0:00:04s\n",
      "epoch 36 | loss: 0.06843 | val_0_mse: 2.46103 |  0:00:04s\n",
      "epoch 37 | loss: 0.07413 | val_0_mse: 2.58007 |  0:00:05s\n",
      "epoch 38 | loss: 0.0681  | val_0_mse: 2.66202 |  0:00:05s\n",
      "epoch 39 | loss: 0.06611 | val_0_mse: 2.56675 |  0:00:05s\n",
      "epoch 40 | loss: 0.06594 | val_0_mse: 2.3474  |  0:00:05s\n",
      "epoch 41 | loss: 0.06735 | val_0_mse: 2.1423  |  0:00:06s\n",
      "epoch 42 | loss: 0.06757 | val_0_mse: 1.90924 |  0:00:07s\n",
      "epoch 43 | loss: 0.08904 | val_0_mse: 1.64567 |  0:00:07s\n",
      "epoch 44 | loss: 0.06626 | val_0_mse: 2.23202 |  0:00:07s\n",
      "epoch 45 | loss: 0.07017 | val_0_mse: 1.58248 |  0:00:07s\n",
      "epoch 46 | loss: 0.07699 | val_0_mse: 0.91003 |  0:00:07s\n",
      "epoch 47 | loss: 0.06772 | val_0_mse: 0.46126 |  0:00:07s\n",
      "epoch 48 | loss: 0.07148 | val_0_mse: 0.2839  |  0:00:07s\n",
      "epoch 49 | loss: 0.06865 | val_0_mse: 0.30012 |  0:00:08s\n",
      "epoch 50 | loss: 0.06846 | val_0_mse: 0.70185 |  0:00:08s\n",
      "epoch 51 | loss: 0.06914 | val_0_mse: 0.47291 |  0:00:08s\n",
      "epoch 52 | loss: 0.06836 | val_0_mse: 0.2909  |  0:00:08s\n",
      "epoch 53 | loss: 0.06506 | val_0_mse: 0.54918 |  0:00:08s\n",
      "epoch 54 | loss: 0.06406 | val_0_mse: 0.52663 |  0:00:08s\n",
      "epoch 55 | loss: 0.06414 | val_0_mse: 0.4663  |  0:00:08s\n",
      "epoch 56 | loss: 0.06554 | val_0_mse: 0.38673 |  0:00:08s\n",
      "epoch 57 | loss: 0.06428 | val_0_mse: 0.33562 |  0:00:08s\n",
      "epoch 58 | loss: 0.06439 | val_0_mse: 0.17283 |  0:00:09s\n",
      "epoch 59 | loss: 0.06269 | val_0_mse: 0.1457  |  0:00:09s\n",
      "epoch 60 | loss: 0.06344 | val_0_mse: 0.16201 |  0:00:09s\n",
      "epoch 61 | loss: 0.06437 | val_0_mse: 0.22126 |  0:00:09s\n",
      "epoch 62 | loss: 0.0651  | val_0_mse: 0.38207 |  0:00:09s\n",
      "epoch 63 | loss: 0.0619  | val_0_mse: 0.5333  |  0:00:09s\n",
      "epoch 64 | loss: 0.06372 | val_0_mse: 0.68967 |  0:00:10s\n",
      "epoch 65 | loss: 0.06326 | val_0_mse: 0.90645 |  0:00:10s\n",
      "epoch 66 | loss: 0.06227 | val_0_mse: 1.14344 |  0:00:10s\n",
      "epoch 67 | loss: 0.06542 | val_0_mse: 1.38027 |  0:00:10s\n",
      "epoch 68 | loss: 0.06268 | val_0_mse: 1.77743 |  0:00:10s\n",
      "epoch 69 | loss: 0.06319 | val_0_mse: 2.11854 |  0:00:10s\n",
      "epoch 70 | loss: 0.06111 | val_0_mse: 2.74294 |  0:00:10s\n",
      "epoch 71 | loss: 0.06062 | val_0_mse: 3.29141 |  0:00:11s\n",
      "epoch 72 | loss: 0.05949 | val_0_mse: 3.92788 |  0:00:11s\n",
      "epoch 73 | loss: 0.06114 | val_0_mse: 4.52438 |  0:00:11s\n",
      "epoch 74 | loss: 0.06209 | val_0_mse: 4.95496 |  0:00:11s\n",
      "epoch 75 | loss: 0.06204 | val_0_mse: 5.07713 |  0:00:11s\n",
      "epoch 76 | loss: 0.06112 | val_0_mse: 4.81365 |  0:00:11s\n",
      "epoch 77 | loss: 0.06077 | val_0_mse: 4.70472 |  0:00:11s\n",
      "epoch 78 | loss: 0.06034 | val_0_mse: 4.18461 |  0:00:12s\n",
      "epoch 79 | loss: 0.0619  | val_0_mse: 3.80337 |  0:00:12s\n",
      "epoch 80 | loss: 0.05947 | val_0_mse: 3.14873 |  0:00:12s\n",
      "epoch 81 | loss: 0.05976 | val_0_mse: 2.42478 |  0:00:12s\n",
      "epoch 82 | loss: 0.06227 | val_0_mse: 1.82248 |  0:00:12s\n",
      "epoch 83 | loss: 0.0582  | val_0_mse: 1.16242 |  0:00:13s\n",
      "epoch 84 | loss: 0.06012 | val_0_mse: 0.72523 |  0:00:13s\n",
      "epoch 85 | loss: 0.05915 | val_0_mse: 0.40884 |  0:00:13s\n",
      "epoch 86 | loss: 0.0606  | val_0_mse: 0.3056  |  0:00:13s\n",
      "epoch 87 | loss: 0.06022 | val_0_mse: 0.26585 |  0:00:13s\n",
      "epoch 88 | loss: 0.05949 | val_0_mse: 0.22093 |  0:00:14s\n",
      "epoch 89 | loss: 0.05903 | val_0_mse: 0.20052 |  0:00:14s\n",
      "epoch 90 | loss: 0.06021 | val_0_mse: 0.18784 |  0:00:14s\n",
      "epoch 91 | loss: 0.05813 | val_0_mse: 0.1732  |  0:00:14s\n",
      "epoch 92 | loss: 0.06135 | val_0_mse: 0.19023 |  0:00:14s\n",
      "epoch 93 | loss: 0.05949 | val_0_mse: 0.20368 |  0:00:14s\n",
      "epoch 94 | loss: 0.05842 | val_0_mse: 0.21619 |  0:00:14s\n",
      "epoch 95 | loss: 0.05995 | val_0_mse: 0.25326 |  0:00:14s\n",
      "epoch 96 | loss: 0.05739 | val_0_mse: 0.24339 |  0:00:15s\n",
      "epoch 97 | loss: 0.0573  | val_0_mse: 0.23701 |  0:00:15s\n",
      "epoch 98 | loss: 0.05686 | val_0_mse: 0.25426 |  0:00:15s\n",
      "epoch 99 | loss: 0.05851 | val_0_mse: 0.26946 |  0:00:15s\n",
      "epoch 100| loss: 0.05757 | val_0_mse: 0.23791 |  0:00:15s\n",
      "epoch 101| loss: 0.05864 | val_0_mse: 0.28005 |  0:00:15s\n",
      "epoch 102| loss: 0.05687 | val_0_mse: 0.31144 |  0:00:15s\n",
      "epoch 103| loss: 0.05812 | val_0_mse: 0.3746  |  0:00:16s\n",
      "epoch 104| loss: 0.06014 | val_0_mse: 0.51099 |  0:00:16s\n",
      "epoch 105| loss: 0.0579  | val_0_mse: 0.59771 |  0:00:16s\n",
      "epoch 106| loss: 0.05974 | val_0_mse: 0.61671 |  0:00:16s\n",
      "epoch 107| loss: 0.05828 | val_0_mse: 0.68188 |  0:00:16s\n",
      "epoch 108| loss: 0.05861 | val_0_mse: 0.80472 |  0:00:16s\n",
      "epoch 109| loss: 0.05906 | val_0_mse: 1.00605 |  0:00:16s\n",
      "epoch 110| loss: 0.05878 | val_0_mse: 1.17459 |  0:00:17s\n",
      "epoch 111| loss: 0.05917 | val_0_mse: 1.28487 |  0:00:17s\n",
      "epoch 112| loss: 0.05616 | val_0_mse: 1.23193 |  0:00:17s\n",
      "epoch 113| loss: 0.05992 | val_0_mse: 1.20195 |  0:00:17s\n",
      "epoch 114| loss: 0.05751 | val_0_mse: 1.19559 |  0:00:17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 115| loss: 0.05547 | val_0_mse: 1.17006 |  0:00:18s\n",
      "epoch 116| loss: 0.05813 | val_0_mse: 1.13476 |  0:00:18s\n",
      "epoch 117| loss: 0.05641 | val_0_mse: 1.11543 |  0:00:18s\n",
      "epoch 118| loss: 0.05984 | val_0_mse: 0.97341 |  0:00:18s\n",
      "epoch 119| loss: 0.057   | val_0_mse: 0.91287 |  0:00:19s\n",
      "\n",
      "Early stopping occurred at epoch 119 with best_epoch = 59 and best_val_0_mse = 0.1457\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.66922 | val_0_mse: 70.84547|  0:00:00s\n",
      "epoch 1  | loss: 1.12922 | val_0_mse: 152.03895|  0:00:00s\n",
      "epoch 2  | loss: 0.63774 | val_0_mse: 210.57359|  0:00:00s\n",
      "epoch 3  | loss: 0.46218 | val_0_mse: 1516.38386|  0:00:00s\n",
      "epoch 4  | loss: 0.34588 | val_0_mse: 674.88739|  0:00:00s\n",
      "epoch 5  | loss: 0.26036 | val_0_mse: 528.45688|  0:00:01s\n",
      "epoch 6  | loss: 0.25878 | val_0_mse: 149.79578|  0:00:01s\n",
      "epoch 7  | loss: 0.2102  | val_0_mse: 17.40091|  0:00:01s\n",
      "epoch 8  | loss: 0.13849 | val_0_mse: 208.91413|  0:00:01s\n",
      "epoch 9  | loss: 0.13555 | val_0_mse: 38.33248|  0:00:01s\n",
      "epoch 10 | loss: 0.14819 | val_0_mse: 40.06995|  0:00:01s\n",
      "epoch 11 | loss: 0.2213  | val_0_mse: 35.25215|  0:00:01s\n",
      "epoch 12 | loss: 0.12262 | val_0_mse: 49.72126|  0:00:02s\n",
      "epoch 13 | loss: 0.11378 | val_0_mse: 36.48847|  0:00:02s\n",
      "epoch 14 | loss: 0.11171 | val_0_mse: 30.55913|  0:00:02s\n",
      "epoch 15 | loss: 0.10814 | val_0_mse: 26.52171|  0:00:02s\n",
      "epoch 16 | loss: 0.09566 | val_0_mse: 48.60114|  0:00:02s\n",
      "epoch 17 | loss: 0.08469 | val_0_mse: 10.39318|  0:00:03s\n",
      "epoch 18 | loss: 0.09427 | val_0_mse: 5.19719 |  0:00:03s\n",
      "epoch 19 | loss: 0.08649 | val_0_mse: 21.41404|  0:00:03s\n",
      "epoch 20 | loss: 0.08106 | val_0_mse: 43.54736|  0:00:03s\n",
      "epoch 21 | loss: 0.07903 | val_0_mse: 49.51616|  0:00:03s\n",
      "epoch 22 | loss: 0.08485 | val_0_mse: 51.62145|  0:00:04s\n",
      "epoch 23 | loss: 0.07934 | val_0_mse: 42.06585|  0:00:04s\n",
      "epoch 24 | loss: 0.0785  | val_0_mse: 16.16931|  0:00:04s\n",
      "epoch 25 | loss: 0.07418 | val_0_mse: 21.04339|  0:00:04s\n",
      "epoch 26 | loss: 0.07748 | val_0_mse: 18.15203|  0:00:04s\n",
      "epoch 27 | loss: 0.07038 | val_0_mse: 8.42545 |  0:00:05s\n",
      "epoch 28 | loss: 0.07615 | val_0_mse: 0.71175 |  0:00:05s\n",
      "epoch 29 | loss: 0.07102 | val_0_mse: 0.57523 |  0:00:06s\n",
      "epoch 30 | loss: 0.07359 | val_0_mse: 0.45384 |  0:00:06s\n",
      "epoch 31 | loss: 0.07107 | val_0_mse: 0.35123 |  0:00:06s\n",
      "epoch 32 | loss: 0.07044 | val_0_mse: 0.31173 |  0:00:06s\n",
      "epoch 33 | loss: 0.06916 | val_0_mse: 0.25353 |  0:00:06s\n",
      "epoch 34 | loss: 0.06985 | val_0_mse: 0.64133 |  0:00:07s\n",
      "epoch 35 | loss: 0.06844 | val_0_mse: 0.16283 |  0:00:07s\n",
      "epoch 36 | loss: 0.06966 | val_0_mse: 0.1336  |  0:00:07s\n",
      "epoch 37 | loss: 0.06626 | val_0_mse: 0.12376 |  0:00:07s\n",
      "epoch 38 | loss: 0.06631 | val_0_mse: 0.1158  |  0:00:07s\n",
      "epoch 39 | loss: 0.06558 | val_0_mse: 0.11125 |  0:00:08s\n",
      "epoch 40 | loss: 0.06624 | val_0_mse: 0.11902 |  0:00:08s\n",
      "epoch 41 | loss: 0.06869 | val_0_mse: 0.16226 |  0:00:08s\n",
      "epoch 42 | loss: 0.0643  | val_0_mse: 0.12755 |  0:00:08s\n",
      "epoch 43 | loss: 0.06517 | val_0_mse: 0.12632 |  0:00:08s\n",
      "epoch 44 | loss: 0.06542 | val_0_mse: 0.1263  |  0:00:09s\n",
      "epoch 45 | loss: 0.06369 | val_0_mse: 0.12821 |  0:00:09s\n",
      "epoch 46 | loss: 0.06769 | val_0_mse: 0.18465 |  0:00:09s\n",
      "epoch 47 | loss: 0.06944 | val_0_mse: 0.1476  |  0:00:09s\n",
      "epoch 48 | loss: 0.06615 | val_0_mse: 0.14814 |  0:00:09s\n",
      "epoch 49 | loss: 0.06344 | val_0_mse: 0.12929 |  0:00:09s\n",
      "epoch 50 | loss: 0.06482 | val_0_mse: 0.13094 |  0:00:10s\n",
      "epoch 51 | loss: 0.06487 | val_0_mse: 0.14617 |  0:00:10s\n",
      "epoch 52 | loss: 0.06697 | val_0_mse: 0.14303 |  0:00:10s\n",
      "epoch 53 | loss: 0.06503 | val_0_mse: 0.15234 |  0:00:10s\n",
      "epoch 54 | loss: 0.06534 | val_0_mse: 0.14043 |  0:00:10s\n",
      "epoch 55 | loss: 0.06422 | val_0_mse: 0.14808 |  0:00:10s\n",
      "epoch 56 | loss: 0.06315 | val_0_mse: 0.1795  |  0:00:11s\n",
      "epoch 57 | loss: 0.06251 | val_0_mse: 0.27992 |  0:00:11s\n",
      "epoch 58 | loss: 0.06427 | val_0_mse: 0.32968 |  0:00:11s\n",
      "epoch 59 | loss: 0.06348 | val_0_mse: 0.32977 |  0:00:11s\n",
      "epoch 60 | loss: 0.06212 | val_0_mse: 0.439   |  0:00:11s\n",
      "epoch 61 | loss: 0.06313 | val_0_mse: 0.36328 |  0:00:11s\n",
      "epoch 62 | loss: 0.06401 | val_0_mse: 0.49583 |  0:00:11s\n",
      "epoch 63 | loss: 0.05957 | val_0_mse: 0.4761  |  0:00:11s\n",
      "epoch 64 | loss: 0.06271 | val_0_mse: 0.47602 |  0:00:12s\n",
      "epoch 65 | loss: 0.06158 | val_0_mse: 0.4889  |  0:00:12s\n",
      "epoch 66 | loss: 0.06366 | val_0_mse: 0.49105 |  0:00:12s\n",
      "epoch 67 | loss: 0.06392 | val_0_mse: 0.40262 |  0:00:12s\n",
      "epoch 68 | loss: 0.06228 | val_0_mse: 0.35351 |  0:00:12s\n",
      "epoch 69 | loss: 0.06284 | val_0_mse: 0.32011 |  0:00:13s\n",
      "epoch 70 | loss: 0.06034 | val_0_mse: 0.30425 |  0:00:13s\n",
      "epoch 71 | loss: 0.06027 | val_0_mse: 0.28029 |  0:00:13s\n",
      "epoch 72 | loss: 0.06143 | val_0_mse: 0.27129 |  0:00:13s\n",
      "epoch 73 | loss: 0.06049 | val_0_mse: 0.32409 |  0:00:13s\n",
      "epoch 74 | loss: 0.06142 | val_0_mse: 0.24936 |  0:00:13s\n",
      "epoch 75 | loss: 0.06027 | val_0_mse: 0.24589 |  0:00:13s\n",
      "epoch 76 | loss: 0.05744 | val_0_mse: 0.26204 |  0:00:14s\n",
      "epoch 77 | loss: 0.06044 | val_0_mse: 0.49412 |  0:00:14s\n",
      "epoch 78 | loss: 0.0582  | val_0_mse: 0.38194 |  0:00:14s\n",
      "epoch 79 | loss: 0.0621  | val_0_mse: 0.49946 |  0:00:14s\n",
      "epoch 80 | loss: 0.05758 | val_0_mse: 0.43634 |  0:00:15s\n",
      "epoch 81 | loss: 0.05773 | val_0_mse: 0.64821 |  0:00:15s\n",
      "epoch 82 | loss: 0.05795 | val_0_mse: 0.48118 |  0:00:15s\n",
      "epoch 83 | loss: 0.05883 | val_0_mse: 0.36606 |  0:00:15s\n",
      "epoch 84 | loss: 0.05783 | val_0_mse: 0.39096 |  0:00:15s\n",
      "epoch 85 | loss: 0.0584  | val_0_mse: 0.36146 |  0:00:15s\n",
      "epoch 86 | loss: 0.05783 | val_0_mse: 0.3511  |  0:00:15s\n",
      "epoch 87 | loss: 0.06028 | val_0_mse: 0.34869 |  0:00:15s\n",
      "epoch 88 | loss: 0.05715 | val_0_mse: 0.27755 |  0:00:15s\n",
      "epoch 89 | loss: 0.0594  | val_0_mse: 0.25401 |  0:00:16s\n",
      "epoch 90 | loss: 0.06204 | val_0_mse: 0.23006 |  0:00:16s\n",
      "epoch 91 | loss: 0.05894 | val_0_mse: 0.19859 |  0:00:16s\n",
      "epoch 92 | loss: 0.06142 | val_0_mse: 0.18139 |  0:00:16s\n",
      "epoch 93 | loss: 0.05687 | val_0_mse: 0.21065 |  0:00:16s\n",
      "epoch 94 | loss: 0.05766 | val_0_mse: 0.18054 |  0:00:16s\n",
      "epoch 95 | loss: 0.05715 | val_0_mse: 0.1453  |  0:00:16s\n",
      "epoch 96 | loss: 0.05684 | val_0_mse: 0.15552 |  0:00:17s\n",
      "epoch 97 | loss: 0.05807 | val_0_mse: 0.1361  |  0:00:17s\n",
      "epoch 98 | loss: 0.0601  | val_0_mse: 0.12176 |  0:00:17s\n",
      "epoch 99 | loss: 0.0563  | val_0_mse: 0.13454 |  0:00:17s\n",
      "\n",
      "Early stopping occurred at epoch 99 with best_epoch = 39 and best_val_0_mse = 0.11125\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "fold = KFold(n_splits=5)\n",
    "n_iter=0\n",
    "cv_rmse = []\n",
    "tab_mean=0\n",
    "for train_index, test_index in fold.split(X_data, y_data):\n",
    "    n_iter += 1\n",
    "    X_train= X_data.iloc[train_index]\n",
    "    X_test= X_data.iloc[test_index]\n",
    "    y_train= y_data.iloc[train_index]\n",
    "    y_tests= y_data.iloc[test_index]\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.reshape(np.array(y_train), (-1,1))\n",
    "    X_test  = np.array(X_test)\n",
    "    y_test  = np.reshape(np.array(y_tests), (-1,1))\n",
    "\n",
    "    tab = TabNetRegressor(seed=0)\n",
    "    tab.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        patience=60,max_epochs=500,\n",
    "        )\n",
    "    preds = tab.predict(X_test)\n",
    "    rmse=math.sqrt(mean_squared_error(preds, y_tests))\n",
    "    cv_rmse.append(rmse)\n",
    "    tab_mean += rmse / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "22652049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 0.5213858577612712\n",
      "2번째 0.3998285666455954\n",
      "3번째 0.34750785389061006\n",
      "4번째 0.38170194153542547\n",
      "5번째 0.33354043310519715\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    rmse=cv_rmse[i]\n",
    "    print(f'{i+1}번째 {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "62231b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3967929305876199"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ef7ce",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "be4204f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f5ba4306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |    eta    |   gamma   | max_depth | min_ch... | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.2813  \u001b[0m | \u001b[0m 0.6293  \u001b[0m | \u001b[0m 0.243   \u001b[0m | \u001b[0m 0.6028  \u001b[0m | \u001b[0m 5.18    \u001b[0m | \u001b[0m 6.695   \u001b[0m | \u001b[0m 0.8938  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.2768  \u001b[0m | \u001b[95m 0.5626  \u001b[0m | \u001b[95m 0.2784  \u001b[0m | \u001b[95m 0.9637  \u001b[0m | \u001b[95m 4.534   \u001b[0m | \u001b[95m 8.167   \u001b[0m | \u001b[95m 0.8587  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.3105  \u001b[0m | \u001b[0m 0.6408  \u001b[0m | \u001b[0m 0.2851  \u001b[0m | \u001b[0m 0.07104 \u001b[0m | \u001b[0m 3.349   \u001b[0m | \u001b[0m 5.081   \u001b[0m | \u001b[0m 0.9498  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-0.2765  \u001b[0m | \u001b[95m 0.596   \u001b[0m | \u001b[95m 0.244   \u001b[0m | \u001b[95m 0.966   \u001b[0m | \u001b[95m 5.548   \u001b[0m | \u001b[95m 7.962   \u001b[0m | \u001b[95m 0.8578  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.3129  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 5.213   \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 0.7     \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m-0.2759  \u001b[0m | \u001b[95m 0.5346  \u001b[0m | \u001b[95m 0.2854  \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 5.002   \u001b[0m | \u001b[95m 7.684   \u001b[0m | \u001b[95m 0.8903  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.2765  \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 6.055   \u001b[0m | \u001b[0m 7.303   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m-0.2758  \u001b[0m | \u001b[95m 0.3     \u001b[0m | \u001b[95m 0.3     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 3.534   \u001b[0m | \u001b[95m 7.955   \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from bayes_opt import BayesianOptimization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "\n",
    "def xgb_evaluate(max_depth, gamma, colsample_bytree,min_child_weight,eta,subsample):\n",
    "    params = {'eval_metric': 'rmse',\n",
    "              'objective':'reg:squarederror',\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'max_depth': int(max_depth),\n",
    "              'subsample': subsample,\n",
    "              'eta': eta,\n",
    "              'gamma': gamma,\n",
    "              'colsample_bytree': colsample_bytree}\n",
    "    # Used around 1000 boosting rounds in the full model\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n",
    "    \n",
    "    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n",
    "    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]\n",
    "\n",
    "xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (3, 7), \n",
    "                                             'gamma': (0, 1),\n",
    "                                             'colsample_bytree': (0.3, 0.9),\n",
    "                                            'min_child_weight': (5, 9),\n",
    "                                            'eta':(0.1, 0.3),\n",
    "                                            'subsample':(0.7, 1.0)}, random_state=0)\n",
    "# Use the expected improvement acquisition function to handle negative numbers\n",
    "# Optimally needs quite a few more initiation points and number of iterations\n",
    "xgb_bo.maximize(init_points=3, n_iter=5, acq='ei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "05c8a963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:26:32] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopings } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.33421\teval-rmse:0.34015\n",
      "[1]\ttrain-rmse:0.32325\teval-rmse:0.32776\n",
      "[2]\ttrain-rmse:0.31478\teval-rmse:0.31761\n",
      "[3]\ttrain-rmse:0.30801\teval-rmse:0.30891\n",
      "[4]\ttrain-rmse:0.30230\teval-rmse:0.30133\n",
      "[5]\ttrain-rmse:0.29795\teval-rmse:0.29538\n",
      "[6]\ttrain-rmse:0.29370\teval-rmse:0.28935\n",
      "[7]\ttrain-rmse:0.28899\teval-rmse:0.28368\n",
      "[8]\ttrain-rmse:0.28542\teval-rmse:0.27897\n",
      "[9]\ttrain-rmse:0.28359\teval-rmse:0.27606\n",
      "[10]\ttrain-rmse:0.28202\teval-rmse:0.27344\n",
      "[11]\ttrain-rmse:0.28083\teval-rmse:0.27138\n",
      "[12]\ttrain-rmse:0.27985\teval-rmse:0.26959\n",
      "[13]\ttrain-rmse:0.27920\teval-rmse:0.26836\n",
      "[14]\ttrain-rmse:0.27789\teval-rmse:0.26840\n",
      "[15]\ttrain-rmse:0.27718\teval-rmse:0.26695\n",
      "[16]\ttrain-rmse:0.27670\teval-rmse:0.26590\n",
      "[17]\ttrain-rmse:0.27623\teval-rmse:0.26482\n",
      "[18]\ttrain-rmse:0.27600\teval-rmse:0.26427\n",
      "[19]\ttrain-rmse:0.27568\teval-rmse:0.26340\n",
      "[20]\ttrain-rmse:0.27552\teval-rmse:0.26296\n",
      "[21]\ttrain-rmse:0.27530\teval-rmse:0.26231\n",
      "[22]\ttrain-rmse:0.27391\teval-rmse:0.26177\n",
      "[23]\ttrain-rmse:0.27374\teval-rmse:0.26122\n",
      "[24]\ttrain-rmse:0.27217\teval-rmse:0.26065\n",
      "[25]\ttrain-rmse:0.27205\teval-rmse:0.26017\n",
      "[26]\ttrain-rmse:0.27198\teval-rmse:0.25986\n",
      "[27]\ttrain-rmse:0.27088\teval-rmse:0.25949\n",
      "[28]\ttrain-rmse:0.27084\teval-rmse:0.25926\n",
      "[29]\ttrain-rmse:0.27085\teval-rmse:0.25933\n",
      "[30]\ttrain-rmse:0.27084\teval-rmse:0.25929\n",
      "[31]\ttrain-rmse:0.27082\teval-rmse:0.25915\n",
      "[32]\ttrain-rmse:0.27084\teval-rmse:0.25925\n",
      "[33]\ttrain-rmse:0.27082\teval-rmse:0.25914\n",
      "[34]\ttrain-rmse:0.27080\teval-rmse:0.25901\n",
      "[35]\ttrain-rmse:0.27078\teval-rmse:0.25886\n",
      "[36]\ttrain-rmse:0.27078\teval-rmse:0.25878\n",
      "[37]\ttrain-rmse:0.27079\teval-rmse:0.25893\n",
      "[38]\ttrain-rmse:0.27080\teval-rmse:0.25897\n",
      "[39]\ttrain-rmse:0.26872\teval-rmse:0.25754\n",
      "[40]\ttrain-rmse:0.26872\teval-rmse:0.25758\n",
      "[41]\ttrain-rmse:0.26871\teval-rmse:0.25748\n",
      "[42]\ttrain-rmse:0.26871\teval-rmse:0.25746\n",
      "[43]\ttrain-rmse:0.26870\teval-rmse:0.25737\n",
      "[44]\ttrain-rmse:0.26870\teval-rmse:0.25733\n",
      "[45]\ttrain-rmse:0.26869\teval-rmse:0.25726\n",
      "[46]\ttrain-rmse:0.26870\teval-rmse:0.25738\n",
      "[47]\ttrain-rmse:0.26871\teval-rmse:0.25747\n",
      "[48]\ttrain-rmse:0.26870\teval-rmse:0.25742\n",
      "[49]\ttrain-rmse:0.26870\teval-rmse:0.25741\n",
      "[50]\ttrain-rmse:0.26870\teval-rmse:0.25734\n",
      "[51]\ttrain-rmse:0.26870\teval-rmse:0.25738\n",
      "[52]\ttrain-rmse:0.26870\teval-rmse:0.25732\n",
      "[53]\ttrain-rmse:0.26870\teval-rmse:0.25737\n",
      "[54]\ttrain-rmse:0.26869\teval-rmse:0.25725\n",
      "[55]\ttrain-rmse:0.26869\teval-rmse:0.25727\n",
      "[56]\ttrain-rmse:0.26868\teval-rmse:0.25710\n",
      "[57]\ttrain-rmse:0.26868\teval-rmse:0.25696\n",
      "[58]\ttrain-rmse:0.26868\teval-rmse:0.25695\n",
      "[59]\ttrain-rmse:0.26868\teval-rmse:0.25699\n",
      "[60]\ttrain-rmse:0.26868\teval-rmse:0.25692\n",
      "[61]\ttrain-rmse:0.26868\teval-rmse:0.25701\n",
      "[62]\ttrain-rmse:0.26868\teval-rmse:0.25695\n",
      "[63]\ttrain-rmse:0.26868\teval-rmse:0.25698\n",
      "[64]\ttrain-rmse:0.26868\teval-rmse:0.25705\n",
      "[65]\ttrain-rmse:0.26868\teval-rmse:0.25701\n",
      "[66]\ttrain-rmse:0.26868\teval-rmse:0.25703\n",
      "[67]\ttrain-rmse:0.26868\teval-rmse:0.25688\n",
      "[68]\ttrain-rmse:0.26868\teval-rmse:0.25697\n",
      "[69]\ttrain-rmse:0.26868\teval-rmse:0.25681\n",
      "[70]\ttrain-rmse:0.26868\teval-rmse:0.25684\n",
      "[71]\ttrain-rmse:0.26869\teval-rmse:0.25672\n",
      "[72]\ttrain-rmse:0.26869\teval-rmse:0.25676\n",
      "[73]\ttrain-rmse:0.26868\teval-rmse:0.25677\n",
      "[74]\ttrain-rmse:0.26868\teval-rmse:0.25678\n",
      "[75]\ttrain-rmse:0.26868\teval-rmse:0.25677\n",
      "[76]\ttrain-rmse:0.26869\teval-rmse:0.25676\n",
      "[77]\ttrain-rmse:0.26868\teval-rmse:0.25680\n",
      "[78]\ttrain-rmse:0.26868\teval-rmse:0.25677\n",
      "[79]\ttrain-rmse:0.26868\teval-rmse:0.25681\n",
      "[80]\ttrain-rmse:0.26868\teval-rmse:0.25684\n",
      "[81]\ttrain-rmse:0.26869\teval-rmse:0.25668\n",
      "[82]\ttrain-rmse:0.26868\teval-rmse:0.25684\n",
      "[83]\ttrain-rmse:0.26868\teval-rmse:0.25690\n",
      "[84]\ttrain-rmse:0.26868\teval-rmse:0.25701\n",
      "[85]\ttrain-rmse:0.26868\teval-rmse:0.25699\n",
      "[86]\ttrain-rmse:0.26868\teval-rmse:0.25695\n",
      "[87]\ttrain-rmse:0.26868\teval-rmse:0.25702\n",
      "[88]\ttrain-rmse:0.26868\teval-rmse:0.25692\n",
      "[89]\ttrain-rmse:0.26868\teval-rmse:0.25693\n",
      "[90]\ttrain-rmse:0.26868\teval-rmse:0.25688\n",
      "[91]\ttrain-rmse:0.26868\teval-rmse:0.25700\n",
      "[12:26:32] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopings } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.34177\teval-rmse:0.30766\n",
      "[1]\ttrain-rmse:0.32967\teval-rmse:0.29584\n",
      "[2]\ttrain-rmse:0.32048\teval-rmse:0.28590\n",
      "[3]\ttrain-rmse:0.31279\teval-rmse:0.27750\n",
      "[4]\ttrain-rmse:0.30554\teval-rmse:0.27094\n",
      "[5]\ttrain-rmse:0.30017\teval-rmse:0.26628\n",
      "[6]\ttrain-rmse:0.29633\teval-rmse:0.26206\n",
      "[7]\ttrain-rmse:0.29278\teval-rmse:0.25816\n",
      "[8]\ttrain-rmse:0.28934\teval-rmse:0.25515\n",
      "[9]\ttrain-rmse:0.28726\teval-rmse:0.25287\n",
      "[10]\ttrain-rmse:0.28546\teval-rmse:0.25090\n",
      "[11]\ttrain-rmse:0.28391\teval-rmse:0.24921\n",
      "[12]\ttrain-rmse:0.28269\teval-rmse:0.24789\n",
      "[13]\ttrain-rmse:0.28008\teval-rmse:0.24645\n",
      "[14]\ttrain-rmse:0.27927\teval-rmse:0.24560\n",
      "[15]\ttrain-rmse:0.27762\teval-rmse:0.24450\n",
      "[16]\ttrain-rmse:0.27712\teval-rmse:0.24399\n",
      "[17]\ttrain-rmse:0.27599\teval-rmse:0.24383\n",
      "[18]\ttrain-rmse:0.27563\teval-rmse:0.24346\n",
      "[19]\ttrain-rmse:0.27526\teval-rmse:0.24309\n",
      "[20]\ttrain-rmse:0.27372\teval-rmse:0.24250\n",
      "[21]\ttrain-rmse:0.27355\teval-rmse:0.24236\n",
      "[22]\ttrain-rmse:0.27347\teval-rmse:0.24229\n",
      "[23]\ttrain-rmse:0.27341\teval-rmse:0.24224\n",
      "[24]\ttrain-rmse:0.27331\teval-rmse:0.24216\n",
      "[25]\ttrain-rmse:0.27325\teval-rmse:0.24211\n",
      "[26]\ttrain-rmse:0.27326\teval-rmse:0.24212\n",
      "[27]\ttrain-rmse:0.27206\teval-rmse:0.24202\n",
      "[28]\ttrain-rmse:0.27205\teval-rmse:0.24201\n",
      "[29]\ttrain-rmse:0.27201\teval-rmse:0.24199\n",
      "[30]\ttrain-rmse:0.27198\teval-rmse:0.24198\n",
      "[31]\ttrain-rmse:0.27193\teval-rmse:0.24197\n",
      "[32]\ttrain-rmse:0.27194\teval-rmse:0.24197\n",
      "[33]\ttrain-rmse:0.27192\teval-rmse:0.24197\n",
      "[34]\ttrain-rmse:0.27190\teval-rmse:0.24197\n",
      "[35]\ttrain-rmse:0.27191\teval-rmse:0.24197\n",
      "[36]\ttrain-rmse:0.27190\teval-rmse:0.24197\n",
      "[37]\ttrain-rmse:0.27189\teval-rmse:0.24197\n",
      "[38]\ttrain-rmse:0.27188\teval-rmse:0.24198\n",
      "[39]\ttrain-rmse:0.27187\teval-rmse:0.24198\n",
      "[40]\ttrain-rmse:0.27188\teval-rmse:0.24198\n",
      "[41]\ttrain-rmse:0.27188\teval-rmse:0.24198\n",
      "[42]\ttrain-rmse:0.27188\teval-rmse:0.24198\n",
      "[12:26:33] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopings } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.33451\teval-rmse:0.33462\n",
      "[1]\ttrain-rmse:0.32167\teval-rmse:0.32395\n",
      "[2]\ttrain-rmse:0.31307\teval-rmse:0.31583\n",
      "[3]\ttrain-rmse:0.30538\teval-rmse:0.30862\n",
      "[4]\ttrain-rmse:0.29876\teval-rmse:0.30297\n",
      "[5]\ttrain-rmse:0.29420\teval-rmse:0.29875\n",
      "[6]\ttrain-rmse:0.28931\teval-rmse:0.29489\n",
      "[7]\ttrain-rmse:0.28566\teval-rmse:0.29153\n",
      "[8]\ttrain-rmse:0.28218\teval-rmse:0.28965\n",
      "[9]\ttrain-rmse:0.28008\teval-rmse:0.28774\n",
      "[10]\ttrain-rmse:0.27833\teval-rmse:0.28617\n",
      "[11]\ttrain-rmse:0.27677\teval-rmse:0.28478\n",
      "[12]\ttrain-rmse:0.27546\teval-rmse:0.28363\n",
      "[13]\ttrain-rmse:0.27156\teval-rmse:0.28204\n",
      "[14]\ttrain-rmse:0.27068\teval-rmse:0.28131\n",
      "[15]\ttrain-rmse:0.26902\teval-rmse:0.28023\n",
      "[16]\ttrain-rmse:0.26853\teval-rmse:0.27982\n",
      "[17]\ttrain-rmse:0.26809\teval-rmse:0.27946\n",
      "[18]\ttrain-rmse:0.26773\teval-rmse:0.27918\n",
      "[19]\ttrain-rmse:0.26721\teval-rmse:0.27878\n",
      "[20]\ttrain-rmse:0.26700\teval-rmse:0.27863\n",
      "[21]\ttrain-rmse:0.26678\teval-rmse:0.27847\n",
      "[22]\ttrain-rmse:0.26671\teval-rmse:0.27842\n",
      "[23]\ttrain-rmse:0.26663\teval-rmse:0.27836\n",
      "[24]\ttrain-rmse:0.26654\teval-rmse:0.27831\n",
      "[25]\ttrain-rmse:0.26645\teval-rmse:0.27825\n",
      "[26]\ttrain-rmse:0.26644\teval-rmse:0.27824\n",
      "[27]\ttrain-rmse:0.26637\teval-rmse:0.27820\n",
      "[28]\ttrain-rmse:0.26633\teval-rmse:0.27818\n",
      "[29]\ttrain-rmse:0.26627\teval-rmse:0.27815\n",
      "[30]\ttrain-rmse:0.26620\teval-rmse:0.27811\n",
      "[31]\ttrain-rmse:0.26549\teval-rmse:0.27735\n",
      "[32]\ttrain-rmse:0.26548\teval-rmse:0.27734\n",
      "[33]\ttrain-rmse:0.26548\teval-rmse:0.27734\n",
      "[34]\ttrain-rmse:0.26545\teval-rmse:0.27733\n",
      "[35]\ttrain-rmse:0.26546\teval-rmse:0.27734\n",
      "[36]\ttrain-rmse:0.26544\teval-rmse:0.27733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37]\ttrain-rmse:0.26544\teval-rmse:0.27733\n",
      "[38]\ttrain-rmse:0.26544\teval-rmse:0.27733\n",
      "[39]\ttrain-rmse:0.26396\teval-rmse:0.27615\n",
      "[40]\ttrain-rmse:0.26398\teval-rmse:0.27615\n",
      "[41]\ttrain-rmse:0.26397\teval-rmse:0.27615\n",
      "[42]\ttrain-rmse:0.26397\teval-rmse:0.27615\n",
      "[43]\ttrain-rmse:0.26395\teval-rmse:0.27615\n",
      "[44]\ttrain-rmse:0.26393\teval-rmse:0.27617\n",
      "[45]\ttrain-rmse:0.26393\teval-rmse:0.27617\n",
      "[46]\ttrain-rmse:0.26292\teval-rmse:0.27685\n",
      "[47]\ttrain-rmse:0.26293\teval-rmse:0.27684\n",
      "[48]\ttrain-rmse:0.26292\teval-rmse:0.27685\n",
      "[49]\ttrain-rmse:0.26292\teval-rmse:0.27686\n",
      "[12:26:33] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopings } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.32907\teval-rmse:0.35937\n",
      "[1]\ttrain-rmse:0.31780\teval-rmse:0.35048\n",
      "[2]\ttrain-rmse:0.30954\teval-rmse:0.34412\n",
      "[3]\ttrain-rmse:0.30107\teval-rmse:0.33786\n",
      "[4]\ttrain-rmse:0.29480\teval-rmse:0.33334\n",
      "[5]\ttrain-rmse:0.28970\teval-rmse:0.32978\n",
      "[6]\ttrain-rmse:0.28534\teval-rmse:0.32684\n",
      "[7]\ttrain-rmse:0.28074\teval-rmse:0.32260\n",
      "[8]\ttrain-rmse:0.27761\teval-rmse:0.32045\n",
      "[9]\ttrain-rmse:0.27538\teval-rmse:0.31897\n",
      "[10]\ttrain-rmse:0.27357\teval-rmse:0.31780\n",
      "[11]\ttrain-rmse:0.27184\teval-rmse:0.31673\n",
      "[12]\ttrain-rmse:0.27056\teval-rmse:0.31597\n",
      "[13]\ttrain-rmse:0.26929\teval-rmse:0.31527\n",
      "[14]\ttrain-rmse:0.26656\teval-rmse:0.31380\n",
      "[15]\ttrain-rmse:0.26569\teval-rmse:0.31322\n",
      "[16]\ttrain-rmse:0.26510\teval-rmse:0.31284\n",
      "[17]\ttrain-rmse:0.26470\teval-rmse:0.31259\n",
      "[18]\ttrain-rmse:0.26449\teval-rmse:0.31246\n",
      "[19]\ttrain-rmse:0.26390\teval-rmse:0.31212\n",
      "[20]\ttrain-rmse:0.26366\teval-rmse:0.31199\n",
      "[21]\ttrain-rmse:0.26347\teval-rmse:0.31189\n",
      "[22]\ttrain-rmse:0.26337\teval-rmse:0.31185\n",
      "[23]\ttrain-rmse:0.26318\teval-rmse:0.31176\n",
      "[24]\ttrain-rmse:0.26306\teval-rmse:0.31172\n",
      "[25]\ttrain-rmse:0.26296\teval-rmse:0.31168\n",
      "[26]\ttrain-rmse:0.26290\teval-rmse:0.31167\n",
      "[27]\ttrain-rmse:0.26285\teval-rmse:0.31165\n",
      "[28]\ttrain-rmse:0.26279\teval-rmse:0.31164\n",
      "[29]\ttrain-rmse:0.26275\teval-rmse:0.31164\n",
      "[30]\ttrain-rmse:0.26269\teval-rmse:0.31164\n",
      "[31]\ttrain-rmse:0.26265\teval-rmse:0.31165\n",
      "[32]\ttrain-rmse:0.26263\teval-rmse:0.31165\n",
      "[33]\ttrain-rmse:0.26264\teval-rmse:0.31165\n",
      "[34]\ttrain-rmse:0.26262\teval-rmse:0.31166\n",
      "[35]\ttrain-rmse:0.26265\teval-rmse:0.31165\n",
      "[36]\ttrain-rmse:0.26261\teval-rmse:0.31166\n",
      "[37]\ttrain-rmse:0.26261\teval-rmse:0.31166\n",
      "[38]\ttrain-rmse:0.26261\teval-rmse:0.31166\n",
      "[39]\ttrain-rmse:0.26261\teval-rmse:0.31166\n",
      "[12:26:34] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopings } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.33373\teval-rmse:0.33203\n",
      "[1]\ttrain-rmse:0.32189\teval-rmse:0.32141\n",
      "[2]\ttrain-rmse:0.31374\teval-rmse:0.31385\n",
      "[3]\ttrain-rmse:0.30527\teval-rmse:0.30782\n",
      "[4]\ttrain-rmse:0.29798\teval-rmse:0.30179\n",
      "[5]\ttrain-rmse:0.29314\teval-rmse:0.29747\n",
      "[6]\ttrain-rmse:0.28923\teval-rmse:0.29402\n",
      "[7]\ttrain-rmse:0.28485\teval-rmse:0.29094\n",
      "[8]\ttrain-rmse:0.28190\teval-rmse:0.28845\n",
      "[9]\ttrain-rmse:0.27969\teval-rmse:0.28663\n",
      "[10]\ttrain-rmse:0.27760\teval-rmse:0.28495\n",
      "[11]\ttrain-rmse:0.27593\teval-rmse:0.28363\n",
      "[12]\ttrain-rmse:0.27421\teval-rmse:0.28244\n",
      "[13]\ttrain-rmse:0.27306\teval-rmse:0.28159\n",
      "[14]\ttrain-rmse:0.27214\teval-rmse:0.28094\n",
      "[15]\ttrain-rmse:0.27151\teval-rmse:0.28051\n",
      "[16]\ttrain-rmse:0.27092\teval-rmse:0.28013\n",
      "[17]\ttrain-rmse:0.27048\teval-rmse:0.27986\n",
      "[18]\ttrain-rmse:0.27027\teval-rmse:0.27974\n",
      "[19]\ttrain-rmse:0.26992\teval-rmse:0.27954\n",
      "[20]\ttrain-rmse:0.26965\teval-rmse:0.27941\n",
      "[21]\ttrain-rmse:0.26945\teval-rmse:0.27932\n",
      "[22]\ttrain-rmse:0.26933\teval-rmse:0.27927\n",
      "[23]\ttrain-rmse:0.26914\teval-rmse:0.27921\n",
      "[24]\ttrain-rmse:0.26908\teval-rmse:0.27919\n",
      "[25]\ttrain-rmse:0.26895\teval-rmse:0.27916\n",
      "[26]\ttrain-rmse:0.26888\teval-rmse:0.27915\n",
      "[27]\ttrain-rmse:0.26882\teval-rmse:0.27915\n",
      "[28]\ttrain-rmse:0.26872\teval-rmse:0.27916\n",
      "[29]\ttrain-rmse:0.26870\teval-rmse:0.27917\n",
      "[30]\ttrain-rmse:0.26684\teval-rmse:0.27994\n",
      "[31]\ttrain-rmse:0.26681\teval-rmse:0.27996\n",
      "[32]\ttrain-rmse:0.26682\teval-rmse:0.27995\n",
      "[33]\ttrain-rmse:0.26682\teval-rmse:0.27995\n",
      "[34]\ttrain-rmse:0.26677\teval-rmse:0.27999\n",
      "[35]\ttrain-rmse:0.26680\teval-rmse:0.27996\n",
      "[36]\ttrain-rmse:0.26676\teval-rmse:0.28000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x2448 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold = KFold(n_splits=5)\n",
    "n_iter=0\n",
    "cv_xgb = []\n",
    "xgb_mean=0\n",
    "for train_index, test_index in fold.split(X_data, y_data):\n",
    "    n_iter += 1\n",
    "    X_train= X_data.iloc[train_index]\n",
    "    X_test= X_data.iloc[test_index]\n",
    "    y_train= y_data.iloc[train_index]\n",
    "    y_test= y_data.iloc[test_index]\n",
    "    import xgboost as xgb\n",
    "    dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "    params = {\n",
    "             'max_depth': 7,\n",
    "             'eta': 0.1,\n",
    "             'objective':'reg:squarederror',\n",
    "             'eval_metric': 'rmse',\n",
    "             'early_stopings':10,\n",
    "             'gamma': 1.0,\n",
    "             'colsample_bytree':0.9,\n",
    "             'min_child_weight': 6.271,\n",
    "             'subsample': 0.7\n",
    "             }\n",
    "    num_rounds = 500\n",
    "\n",
    "    wlist = [(dtrain, 'train'),(dtest, 'eval')]\n",
    "    xgb =xgb.train(params = params, dtrain=dtrain, num_boost_round=num_rounds, early_stopping_rounds=10, evals=wlist)\n",
    "\n",
    "    pred = xgb.predict(dtest)\n",
    "    xgb_rmse=math.sqrt(mean_squared_error(pred, y_test))\n",
    "    cv_xgb.append(xgb_rmse)\n",
    "    xgb_mean+=xgb_rmse/5\n",
    "\n",
    "\n",
    "\n",
    "    from xgboost import plot_importance #  변수 중요도\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(24, 34))\n",
    "    plot_importance(xgb, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ca73dc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 0.25700037735394493\n",
      "2번째 0.24198167192085426\n",
      "3번째 0.2768580829789449\n",
      "4번째 0.3116602291965178\n",
      "5번째 0.2800133177962446\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    rmse=cv_xgb[i]\n",
    "    print(f'{i+1}번째 {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "71f78b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2735027358493013"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ce793",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d05e2819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "561ededd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | learni... | maxDepth  | min_ch... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.278   \u001b[0m | \u001b[0m 0.7085  \u001b[0m | \u001b[0m 0.2189  \u001b[0m | \u001b[0m 3.001   \u001b[0m | \u001b[0m 6.209   \u001b[0m | \u001b[0m 27.08   \u001b[0m | \u001b[0m 0.8185  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.2798  \u001b[0m | \u001b[0m 0.5931  \u001b[0m | \u001b[0m 0.1102  \u001b[0m | \u001b[0m 5.777   \u001b[0m | \u001b[0m 7.155   \u001b[0m | \u001b[0m 32.8    \u001b[0m | \u001b[0m 0.937   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.2793  \u001b[0m | \u001b[0m 0.6022  \u001b[0m | \u001b[0m 0.2647  \u001b[0m | \u001b[0m 3.192   \u001b[0m | \u001b[0m 7.682   \u001b[0m | \u001b[0m 32.76   \u001b[0m | \u001b[0m 0.9117  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.2813  \u001b[0m | \u001b[0m 0.5702  \u001b[0m | \u001b[0m 0.06745 \u001b[0m | \u001b[0m 8.605   \u001b[0m | \u001b[0m 8.873   \u001b[0m | \u001b[0m 30.58   \u001b[0m | \u001b[0m 0.9385  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.2802  \u001b[0m | \u001b[0m 0.9382  \u001b[0m | \u001b[0m 0.2694  \u001b[0m | \u001b[0m 3.595   \u001b[0m | \u001b[0m 5.156   \u001b[0m | \u001b[0m 27.57   \u001b[0m | \u001b[0m 0.9756  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.281   \u001b[0m | \u001b[0m 0.7807  \u001b[0m | \u001b[0m 0.1189  \u001b[0m | \u001b[0m 6.352   \u001b[0m | \u001b[0m 7.245   \u001b[0m | \u001b[0m 26.89   \u001b[0m | \u001b[0m 0.9289  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.281   \u001b[0m | \u001b[0m 0.7989  \u001b[0m | \u001b[0m 0.1023  \u001b[0m | \u001b[0m 7.641   \u001b[0m | \u001b[0m 6.794   \u001b[0m | \u001b[0m 27.62   \u001b[0m | \u001b[0m 0.8606  \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m-0.2778  \u001b[0m | \u001b[95m 0.7152  \u001b[0m | \u001b[95m 0.1529  \u001b[0m | \u001b[95m 3.376   \u001b[0m | \u001b[95m 6.184   \u001b[0m | \u001b[95m 26.6    \u001b[0m | \u001b[95m 0.8573  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.2786  \u001b[0m | \u001b[0m 0.5114  \u001b[0m | \u001b[0m 0.198   \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 7.345   \u001b[0m | \u001b[0m 26.32   \u001b[0m | \u001b[0m 0.8     \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.2796  \u001b[0m | \u001b[0m 0.8527  \u001b[0m | \u001b[0m 0.2204  \u001b[0m | \u001b[0m 3.121   \u001b[0m | \u001b[0m 5.883   \u001b[0m | \u001b[0m 24.97   \u001b[0m | \u001b[0m 0.9514  \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# LIGHTGBM 베이지안 최적화\n",
    "def modelFitter(colsampleByTree, subsample,maxDepth, num_leaves,learning_rate,min_child_weight):\n",
    "    model = LGBMRegressor(learning_rate=learning_rate, n_estimators=10000, max_depth=maxDepth.astype(\"int32\"), subsample=subsample, colsample_bytree=colsampleByTree,num_leaves=num_leaves.astype(\"int32\"),min_child_weight=min_child_weight)\n",
    "\n",
    "    evalSet  = [(X_test, y_test)]\n",
    "    model.fit(X_train, y_train, eval_metric=\"rmse\", eval_set=evalSet, early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "    bestScore = model.best_score_[list(model.best_score_.keys())[0]]['rmse']\n",
    "\n",
    "    return -bestScore\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'colsampleByTree': (0.5,1.0), 'subsample': (0.8,1.0), 'maxDepth': (3,10), 'num_leaves': (24, 45),'learning_rate':(0.01,0.3),'min_child_weight':(5,9)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=modelFitter,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1)\n",
    "\n",
    "optimizer.maximize(init_points=5,n_iter=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "94f1fdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's rmse: 0.255602\tvalid_0's l2: 0.0653324\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's rmse: 0.253214\tvalid_0's l2: 0.0641174\n",
      "[3]\tvalid_0's rmse: 0.248686\tvalid_0's l2: 0.0618449\n",
      "[4]\tvalid_0's rmse: 0.24652\tvalid_0's l2: 0.0607722\n",
      "[5]\tvalid_0's rmse: 0.245325\tvalid_0's l2: 0.0601846\n",
      "[6]\tvalid_0's rmse: 0.244154\tvalid_0's l2: 0.059611\n",
      "[7]\tvalid_0's rmse: 0.242938\tvalid_0's l2: 0.0590189\n",
      "[8]\tvalid_0's rmse: 0.24082\tvalid_0's l2: 0.0579942\n",
      "[9]\tvalid_0's rmse: 0.241159\tvalid_0's l2: 0.0581577\n",
      "[10]\tvalid_0's rmse: 0.239628\tvalid_0's l2: 0.0574215\n",
      "[11]\tvalid_0's rmse: 0.23921\tvalid_0's l2: 0.0572216\n",
      "[12]\tvalid_0's rmse: 0.238548\tvalid_0's l2: 0.056905\n",
      "[13]\tvalid_0's rmse: 0.23996\tvalid_0's l2: 0.0575809\n",
      "[14]\tvalid_0's rmse: 0.240188\tvalid_0's l2: 0.0576903\n",
      "[15]\tvalid_0's rmse: 0.240176\tvalid_0's l2: 0.0576843\n",
      "[16]\tvalid_0's rmse: 0.240847\tvalid_0's l2: 0.0580071\n",
      "[17]\tvalid_0's rmse: 0.240706\tvalid_0's l2: 0.0579393\n",
      "[18]\tvalid_0's rmse: 0.241256\tvalid_0's l2: 0.0582047\n",
      "[19]\tvalid_0's rmse: 0.241191\tvalid_0's l2: 0.0581731\n",
      "[20]\tvalid_0's rmse: 0.241639\tvalid_0's l2: 0.0583894\n",
      "[21]\tvalid_0's rmse: 0.242906\tvalid_0's l2: 0.0590032\n",
      "[22]\tvalid_0's rmse: 0.243251\tvalid_0's l2: 0.0591709\n",
      "[23]\tvalid_0's rmse: 0.244186\tvalid_0's l2: 0.059627\n",
      "[24]\tvalid_0's rmse: 0.243728\tvalid_0's l2: 0.0594034\n",
      "[25]\tvalid_0's rmse: 0.244266\tvalid_0's l2: 0.0596656\n",
      "[26]\tvalid_0's rmse: 0.244142\tvalid_0's l2: 0.0596053\n",
      "[27]\tvalid_0's rmse: 0.244638\tvalid_0's l2: 0.0598476\n",
      "[28]\tvalid_0's rmse: 0.245257\tvalid_0's l2: 0.0601511\n",
      "[29]\tvalid_0's rmse: 0.245307\tvalid_0's l2: 0.0601757\n",
      "[30]\tvalid_0's rmse: 0.245695\tvalid_0's l2: 0.060366\n",
      "[31]\tvalid_0's rmse: 0.246654\tvalid_0's l2: 0.0608383\n",
      "[32]\tvalid_0's rmse: 0.247085\tvalid_0's l2: 0.0610508\n",
      "[33]\tvalid_0's rmse: 0.247413\tvalid_0's l2: 0.061213\n",
      "[34]\tvalid_0's rmse: 0.247473\tvalid_0's l2: 0.0612427\n",
      "[35]\tvalid_0's rmse: 0.247934\tvalid_0's l2: 0.0614713\n",
      "[36]\tvalid_0's rmse: 0.247931\tvalid_0's l2: 0.0614697\n",
      "[37]\tvalid_0's rmse: 0.248258\tvalid_0's l2: 0.0616318\n",
      "[38]\tvalid_0's rmse: 0.248207\tvalid_0's l2: 0.0616068\n",
      "[39]\tvalid_0's rmse: 0.248942\tvalid_0's l2: 0.0619722\n",
      "[40]\tvalid_0's rmse: 0.249202\tvalid_0's l2: 0.0621019\n",
      "[41]\tvalid_0's rmse: 0.250357\tvalid_0's l2: 0.0626784\n",
      "[42]\tvalid_0's rmse: 0.251334\tvalid_0's l2: 0.0631687\n",
      "[43]\tvalid_0's rmse: 0.252031\tvalid_0's l2: 0.0635196\n",
      "[44]\tvalid_0's rmse: 0.251972\tvalid_0's l2: 0.0634897\n",
      "[45]\tvalid_0's rmse: 0.252763\tvalid_0's l2: 0.0638893\n",
      "[46]\tvalid_0's rmse: 0.252958\tvalid_0's l2: 0.063988\n",
      "[47]\tvalid_0's rmse: 0.254145\tvalid_0's l2: 0.0645894\n",
      "[48]\tvalid_0's rmse: 0.253627\tvalid_0's l2: 0.0643269\n",
      "[49]\tvalid_0's rmse: 0.253905\tvalid_0's l2: 0.0644677\n",
      "[50]\tvalid_0's rmse: 0.254256\tvalid_0's l2: 0.0646462\n",
      "[51]\tvalid_0's rmse: 0.254894\tvalid_0's l2: 0.0649708\n",
      "[52]\tvalid_0's rmse: 0.254941\tvalid_0's l2: 0.064995\n",
      "[53]\tvalid_0's rmse: 0.254914\tvalid_0's l2: 0.064981\n",
      "[54]\tvalid_0's rmse: 0.25471\tvalid_0's l2: 0.0648774\n",
      "[55]\tvalid_0's rmse: 0.254802\tvalid_0's l2: 0.0649241\n",
      "[56]\tvalid_0's rmse: 0.254965\tvalid_0's l2: 0.0650074\n",
      "[57]\tvalid_0's rmse: 0.25532\tvalid_0's l2: 0.0651882\n",
      "[58]\tvalid_0's rmse: 0.256354\tvalid_0's l2: 0.0657172\n",
      "[59]\tvalid_0's rmse: 0.257144\tvalid_0's l2: 0.0661229\n",
      "[60]\tvalid_0's rmse: 0.257628\tvalid_0's l2: 0.0663722\n",
      "[61]\tvalid_0's rmse: 0.257824\tvalid_0's l2: 0.0664732\n",
      "[62]\tvalid_0's rmse: 0.258733\tvalid_0's l2: 0.0669427\n",
      "[63]\tvalid_0's rmse: 0.259125\tvalid_0's l2: 0.0671457\n",
      "[64]\tvalid_0's rmse: 0.259185\tvalid_0's l2: 0.0671768\n",
      "[65]\tvalid_0's rmse: 0.25976\tvalid_0's l2: 0.0674753\n",
      "[66]\tvalid_0's rmse: 0.260666\tvalid_0's l2: 0.0679469\n",
      "[67]\tvalid_0's rmse: 0.26079\tvalid_0's l2: 0.0680116\n",
      "[68]\tvalid_0's rmse: 0.261451\tvalid_0's l2: 0.0683565\n",
      "[69]\tvalid_0's rmse: 0.26215\tvalid_0's l2: 0.0687224\n",
      "[70]\tvalid_0's rmse: 0.262804\tvalid_0's l2: 0.069066\n",
      "[71]\tvalid_0's rmse: 0.262726\tvalid_0's l2: 0.0690251\n",
      "[72]\tvalid_0's rmse: 0.262658\tvalid_0's l2: 0.0689891\n",
      "[73]\tvalid_0's rmse: 0.262605\tvalid_0's l2: 0.0689612\n",
      "[74]\tvalid_0's rmse: 0.262907\tvalid_0's l2: 0.06912\n",
      "[75]\tvalid_0's rmse: 0.26314\tvalid_0's l2: 0.0692427\n",
      "[76]\tvalid_0's rmse: 0.26336\tvalid_0's l2: 0.0693586\n",
      "[77]\tvalid_0's rmse: 0.263588\tvalid_0's l2: 0.0694788\n",
      "[78]\tvalid_0's rmse: 0.263529\tvalid_0's l2: 0.0694473\n",
      "[79]\tvalid_0's rmse: 0.263885\tvalid_0's l2: 0.0696351\n",
      "[80]\tvalid_0's rmse: 0.264207\tvalid_0's l2: 0.0698054\n",
      "[81]\tvalid_0's rmse: 0.264363\tvalid_0's l2: 0.0698878\n",
      "[82]\tvalid_0's rmse: 0.26506\tvalid_0's l2: 0.0702569\n",
      "[83]\tvalid_0's rmse: 0.26488\tvalid_0's l2: 0.0701613\n",
      "[84]\tvalid_0's rmse: 0.264952\tvalid_0's l2: 0.0701997\n",
      "[85]\tvalid_0's rmse: 0.265616\tvalid_0's l2: 0.0705518\n",
      "[86]\tvalid_0's rmse: 0.266077\tvalid_0's l2: 0.070797\n",
      "[87]\tvalid_0's rmse: 0.266598\tvalid_0's l2: 0.0710743\n",
      "[88]\tvalid_0's rmse: 0.266499\tvalid_0's l2: 0.0710216\n",
      "[89]\tvalid_0's rmse: 0.266718\tvalid_0's l2: 0.0711385\n",
      "[90]\tvalid_0's rmse: 0.266696\tvalid_0's l2: 0.0711267\n",
      "[91]\tvalid_0's rmse: 0.266765\tvalid_0's l2: 0.0711634\n",
      "[92]\tvalid_0's rmse: 0.26641\tvalid_0's l2: 0.0709745\n",
      "[93]\tvalid_0's rmse: 0.26614\tvalid_0's l2: 0.0708304\n",
      "[94]\tvalid_0's rmse: 0.267022\tvalid_0's l2: 0.0713007\n",
      "[95]\tvalid_0's rmse: 0.266846\tvalid_0's l2: 0.0712066\n",
      "[96]\tvalid_0's rmse: 0.267428\tvalid_0's l2: 0.0715177\n",
      "[97]\tvalid_0's rmse: 0.26785\tvalid_0's l2: 0.0717434\n",
      "[98]\tvalid_0's rmse: 0.268123\tvalid_0's l2: 0.0718902\n",
      "[99]\tvalid_0's rmse: 0.268805\tvalid_0's l2: 0.0722561\n",
      "[100]\tvalid_0's rmse: 0.268745\tvalid_0's l2: 0.0722238\n",
      "[101]\tvalid_0's rmse: 0.268928\tvalid_0's l2: 0.0723225\n",
      "[102]\tvalid_0's rmse: 0.269234\tvalid_0's l2: 0.0724871\n",
      "[103]\tvalid_0's rmse: 0.26916\tvalid_0's l2: 0.0724473\n",
      "[104]\tvalid_0's rmse: 0.269068\tvalid_0's l2: 0.0723977\n",
      "[105]\tvalid_0's rmse: 0.269229\tvalid_0's l2: 0.072484\n",
      "[106]\tvalid_0's rmse: 0.269648\tvalid_0's l2: 0.0727099\n",
      "[107]\tvalid_0's rmse: 0.27006\tvalid_0's l2: 0.0729322\n",
      "[108]\tvalid_0's rmse: 0.270239\tvalid_0's l2: 0.073029\n",
      "[109]\tvalid_0's rmse: 0.269737\tvalid_0's l2: 0.0727582\n",
      "[110]\tvalid_0's rmse: 0.270194\tvalid_0's l2: 0.0730046\n",
      "[111]\tvalid_0's rmse: 0.270271\tvalid_0's l2: 0.0730463\n",
      "[112]\tvalid_0's rmse: 0.27026\tvalid_0's l2: 0.0730406\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's rmse: 0.238548\tvalid_0's l2: 0.056905\n",
      "[LightGBM] [Warning] Unknown parameter: sumbsample\n",
      "[1]\tvalid_0's rmse: 0.243939\tvalid_0's l2: 0.0595063\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's rmse: 0.242222\tvalid_0's l2: 0.0586715\n",
      "[3]\tvalid_0's rmse: 0.244724\tvalid_0's l2: 0.0598898\n",
      "[4]\tvalid_0's rmse: 0.245979\tvalid_0's l2: 0.0605057\n",
      "[5]\tvalid_0's rmse: 0.244831\tvalid_0's l2: 0.0599421\n",
      "[6]\tvalid_0's rmse: 0.24504\tvalid_0's l2: 0.0600447\n",
      "[7]\tvalid_0's rmse: 0.246189\tvalid_0's l2: 0.0606089\n",
      "[8]\tvalid_0's rmse: 0.246105\tvalid_0's l2: 0.0605676\n",
      "[9]\tvalid_0's rmse: 0.246039\tvalid_0's l2: 0.060535\n",
      "[10]\tvalid_0's rmse: 0.245955\tvalid_0's l2: 0.0604936\n",
      "[11]\tvalid_0's rmse: 0.247138\tvalid_0's l2: 0.061077\n",
      "[12]\tvalid_0's rmse: 0.247525\tvalid_0's l2: 0.0612687\n",
      "[13]\tvalid_0's rmse: 0.247433\tvalid_0's l2: 0.0612229\n",
      "[14]\tvalid_0's rmse: 0.248376\tvalid_0's l2: 0.0616905\n",
      "[15]\tvalid_0's rmse: 0.249617\tvalid_0's l2: 0.0623084\n",
      "[16]\tvalid_0's rmse: 0.250478\tvalid_0's l2: 0.0627391\n",
      "[17]\tvalid_0's rmse: 0.251499\tvalid_0's l2: 0.0632515\n",
      "[18]\tvalid_0's rmse: 0.251267\tvalid_0's l2: 0.0631352\n",
      "[19]\tvalid_0's rmse: 0.25294\tvalid_0's l2: 0.0639789\n",
      "[20]\tvalid_0's rmse: 0.252396\tvalid_0's l2: 0.0637039\n",
      "[21]\tvalid_0's rmse: 0.252531\tvalid_0's l2: 0.0637717\n",
      "[22]\tvalid_0's rmse: 0.252255\tvalid_0's l2: 0.0636327\n",
      "[23]\tvalid_0's rmse: 0.253011\tvalid_0's l2: 0.0640144\n",
      "[24]\tvalid_0's rmse: 0.253092\tvalid_0's l2: 0.0640557\n",
      "[25]\tvalid_0's rmse: 0.252734\tvalid_0's l2: 0.0638743\n",
      "[26]\tvalid_0's rmse: 0.252702\tvalid_0's l2: 0.0638582\n",
      "[27]\tvalid_0's rmse: 0.253204\tvalid_0's l2: 0.0641124\n",
      "[28]\tvalid_0's rmse: 0.254407\tvalid_0's l2: 0.0647229\n",
      "[29]\tvalid_0's rmse: 0.255441\tvalid_0's l2: 0.0652503\n",
      "[30]\tvalid_0's rmse: 0.255383\tvalid_0's l2: 0.0652203\n",
      "[31]\tvalid_0's rmse: 0.256345\tvalid_0's l2: 0.0657127\n",
      "[32]\tvalid_0's rmse: 0.255022\tvalid_0's l2: 0.0650363\n",
      "[33]\tvalid_0's rmse: 0.254486\tvalid_0's l2: 0.0647631\n",
      "[34]\tvalid_0's rmse: 0.255524\tvalid_0's l2: 0.0652927\n",
      "[35]\tvalid_0's rmse: 0.25611\tvalid_0's l2: 0.0655923\n",
      "[36]\tvalid_0's rmse: 0.256157\tvalid_0's l2: 0.0656163\n",
      "[37]\tvalid_0's rmse: 0.256848\tvalid_0's l2: 0.0659709\n",
      "[38]\tvalid_0's rmse: 0.256932\tvalid_0's l2: 0.0660138\n",
      "[39]\tvalid_0's rmse: 0.256774\tvalid_0's l2: 0.0659329\n",
      "[40]\tvalid_0's rmse: 0.25724\tvalid_0's l2: 0.0661723\n",
      "[41]\tvalid_0's rmse: 0.257395\tvalid_0's l2: 0.0662524\n",
      "[42]\tvalid_0's rmse: 0.257296\tvalid_0's l2: 0.0662014\n",
      "[43]\tvalid_0's rmse: 0.257737\tvalid_0's l2: 0.0664283\n",
      "[44]\tvalid_0's rmse: 0.257867\tvalid_0's l2: 0.0664953\n",
      "[45]\tvalid_0's rmse: 0.257525\tvalid_0's l2: 0.0663191\n",
      "[46]\tvalid_0's rmse: 0.257739\tvalid_0's l2: 0.0664295\n",
      "[47]\tvalid_0's rmse: 0.257418\tvalid_0's l2: 0.066264\n",
      "[48]\tvalid_0's rmse: 0.257489\tvalid_0's l2: 0.0663003\n",
      "[49]\tvalid_0's rmse: 0.258904\tvalid_0's l2: 0.0670314\n",
      "[50]\tvalid_0's rmse: 0.259328\tvalid_0's l2: 0.0672508\n",
      "[51]\tvalid_0's rmse: 0.259113\tvalid_0's l2: 0.0671398\n",
      "[52]\tvalid_0's rmse: 0.259304\tvalid_0's l2: 0.0672384\n",
      "[53]\tvalid_0's rmse: 0.259012\tvalid_0's l2: 0.0670872\n",
      "[54]\tvalid_0's rmse: 0.258671\tvalid_0's l2: 0.0669108\n",
      "[55]\tvalid_0's rmse: 0.259373\tvalid_0's l2: 0.0672745\n",
      "[56]\tvalid_0's rmse: 0.259745\tvalid_0's l2: 0.0674675\n",
      "[57]\tvalid_0's rmse: 0.259599\tvalid_0's l2: 0.0673916\n",
      "[58]\tvalid_0's rmse: 0.259183\tvalid_0's l2: 0.0671758\n",
      "[59]\tvalid_0's rmse: 0.25948\tvalid_0's l2: 0.0673301\n",
      "[60]\tvalid_0's rmse: 0.259666\tvalid_0's l2: 0.0674263\n",
      "[61]\tvalid_0's rmse: 0.259832\tvalid_0's l2: 0.0675128\n",
      "[62]\tvalid_0's rmse: 0.259483\tvalid_0's l2: 0.0673315\n",
      "[63]\tvalid_0's rmse: 0.258687\tvalid_0's l2: 0.0669191\n",
      "[64]\tvalid_0's rmse: 0.258853\tvalid_0's l2: 0.0670047\n",
      "[65]\tvalid_0's rmse: 0.258776\tvalid_0's l2: 0.0669649\n",
      "[66]\tvalid_0's rmse: 0.259421\tvalid_0's l2: 0.0672991\n",
      "[67]\tvalid_0's rmse: 0.260366\tvalid_0's l2: 0.0677904\n",
      "[68]\tvalid_0's rmse: 0.260623\tvalid_0's l2: 0.0679246\n",
      "[69]\tvalid_0's rmse: 0.261202\tvalid_0's l2: 0.0682266\n",
      "[70]\tvalid_0's rmse: 0.261146\tvalid_0's l2: 0.0681971\n",
      "[71]\tvalid_0's rmse: 0.261154\tvalid_0's l2: 0.0682015\n",
      "[72]\tvalid_0's rmse: 0.261035\tvalid_0's l2: 0.0681393\n",
      "[73]\tvalid_0's rmse: 0.261265\tvalid_0's l2: 0.0682594\n",
      "[74]\tvalid_0's rmse: 0.261458\tvalid_0's l2: 0.0683601\n",
      "[75]\tvalid_0's rmse: 0.261168\tvalid_0's l2: 0.0682087\n",
      "[76]\tvalid_0's rmse: 0.260848\tvalid_0's l2: 0.0680417\n",
      "[77]\tvalid_0's rmse: 0.260503\tvalid_0's l2: 0.0678618\n",
      "[78]\tvalid_0's rmse: 0.260272\tvalid_0's l2: 0.0677418\n",
      "[79]\tvalid_0's rmse: 0.260775\tvalid_0's l2: 0.0680037\n",
      "[80]\tvalid_0's rmse: 0.260826\tvalid_0's l2: 0.0680302\n",
      "[81]\tvalid_0's rmse: 0.261074\tvalid_0's l2: 0.0681594\n",
      "[82]\tvalid_0's rmse: 0.260851\tvalid_0's l2: 0.0680431\n",
      "[83]\tvalid_0's rmse: 0.260905\tvalid_0's l2: 0.0680712\n",
      "[84]\tvalid_0's rmse: 0.260588\tvalid_0's l2: 0.067906\n",
      "[85]\tvalid_0's rmse: 0.260702\tvalid_0's l2: 0.0679654\n",
      "[86]\tvalid_0's rmse: 0.260852\tvalid_0's l2: 0.068044\n",
      "[87]\tvalid_0's rmse: 0.260961\tvalid_0's l2: 0.0681007\n",
      "[88]\tvalid_0's rmse: 0.261132\tvalid_0's l2: 0.0681898\n",
      "[89]\tvalid_0's rmse: 0.261309\tvalid_0's l2: 0.0682824\n",
      "[90]\tvalid_0's rmse: 0.260872\tvalid_0's l2: 0.0680544\n",
      "[91]\tvalid_0's rmse: 0.261058\tvalid_0's l2: 0.0681514\n",
      "[92]\tvalid_0's rmse: 0.261755\tvalid_0's l2: 0.0685157\n",
      "[93]\tvalid_0's rmse: 0.262132\tvalid_0's l2: 0.0687131\n",
      "[94]\tvalid_0's rmse: 0.262655\tvalid_0's l2: 0.0689879\n",
      "[95]\tvalid_0's rmse: 0.262509\tvalid_0's l2: 0.068911\n",
      "[96]\tvalid_0's rmse: 0.263151\tvalid_0's l2: 0.0692482\n",
      "[97]\tvalid_0's rmse: 0.263588\tvalid_0's l2: 0.0694788\n",
      "[98]\tvalid_0's rmse: 0.264374\tvalid_0's l2: 0.0698936\n",
      "[99]\tvalid_0's rmse: 0.264457\tvalid_0's l2: 0.0699375\n",
      "[100]\tvalid_0's rmse: 0.264299\tvalid_0's l2: 0.0698539\n",
      "[101]\tvalid_0's rmse: 0.264204\tvalid_0's l2: 0.069804\n",
      "[102]\tvalid_0's rmse: 0.264036\tvalid_0's l2: 0.069715\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's rmse: 0.242222\tvalid_0's l2: 0.0586715\n",
      "[LightGBM] [Warning] Unknown parameter: sumbsample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's rmse: 0.277816\tvalid_0's l2: 0.0771816\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's rmse: 0.275496\tvalid_0's l2: 0.0758983\n",
      "[3]\tvalid_0's rmse: 0.275009\tvalid_0's l2: 0.0756297\n",
      "[4]\tvalid_0's rmse: 0.275131\tvalid_0's l2: 0.0756972\n",
      "[5]\tvalid_0's rmse: 0.277675\tvalid_0's l2: 0.0771032\n",
      "[6]\tvalid_0's rmse: 0.277625\tvalid_0's l2: 0.0770758\n",
      "[7]\tvalid_0's rmse: 0.277173\tvalid_0's l2: 0.0768248\n",
      "[8]\tvalid_0's rmse: 0.277297\tvalid_0's l2: 0.0768935\n",
      "[9]\tvalid_0's rmse: 0.278539\tvalid_0's l2: 0.0775841\n",
      "[10]\tvalid_0's rmse: 0.279218\tvalid_0's l2: 0.0779629\n",
      "[11]\tvalid_0's rmse: 0.280141\tvalid_0's l2: 0.0784787\n",
      "[12]\tvalid_0's rmse: 0.280401\tvalid_0's l2: 0.0786246\n",
      "[13]\tvalid_0's rmse: 0.279556\tvalid_0's l2: 0.0781513\n",
      "[14]\tvalid_0's rmse: 0.280892\tvalid_0's l2: 0.0789004\n",
      "[15]\tvalid_0's rmse: 0.281748\tvalid_0's l2: 0.0793818\n",
      "[16]\tvalid_0's rmse: 0.28143\tvalid_0's l2: 0.079203\n",
      "[17]\tvalid_0's rmse: 0.281321\tvalid_0's l2: 0.0791415\n",
      "[18]\tvalid_0's rmse: 0.280919\tvalid_0's l2: 0.0789157\n",
      "[19]\tvalid_0's rmse: 0.280724\tvalid_0's l2: 0.0788059\n",
      "[20]\tvalid_0's rmse: 0.281449\tvalid_0's l2: 0.0792134\n",
      "[21]\tvalid_0's rmse: 0.282176\tvalid_0's l2: 0.0796232\n",
      "[22]\tvalid_0's rmse: 0.282042\tvalid_0's l2: 0.0795476\n",
      "[23]\tvalid_0's rmse: 0.283018\tvalid_0's l2: 0.0800993\n",
      "[24]\tvalid_0's rmse: 0.283432\tvalid_0's l2: 0.0803335\n",
      "[25]\tvalid_0's rmse: 0.284054\tvalid_0's l2: 0.0806868\n",
      "[26]\tvalid_0's rmse: 0.28412\tvalid_0's l2: 0.0807244\n",
      "[27]\tvalid_0's rmse: 0.28455\tvalid_0's l2: 0.0809686\n",
      "[28]\tvalid_0's rmse: 0.284405\tvalid_0's l2: 0.0808861\n",
      "[29]\tvalid_0's rmse: 0.284718\tvalid_0's l2: 0.0810644\n",
      "[30]\tvalid_0's rmse: 0.284412\tvalid_0's l2: 0.0808903\n",
      "[31]\tvalid_0's rmse: 0.285077\tvalid_0's l2: 0.081269\n",
      "[32]\tvalid_0's rmse: 0.285563\tvalid_0's l2: 0.0815465\n",
      "[33]\tvalid_0's rmse: 0.285582\tvalid_0's l2: 0.0815574\n",
      "[34]\tvalid_0's rmse: 0.28541\tvalid_0's l2: 0.081459\n",
      "[35]\tvalid_0's rmse: 0.285207\tvalid_0's l2: 0.0813432\n",
      "[36]\tvalid_0's rmse: 0.285319\tvalid_0's l2: 0.0814072\n",
      "[37]\tvalid_0's rmse: 0.284973\tvalid_0's l2: 0.0812095\n",
      "[38]\tvalid_0's rmse: 0.28504\tvalid_0's l2: 0.0812475\n",
      "[39]\tvalid_0's rmse: 0.286464\tvalid_0's l2: 0.0820615\n",
      "[40]\tvalid_0's rmse: 0.285975\tvalid_0's l2: 0.0817818\n",
      "[41]\tvalid_0's rmse: 0.286485\tvalid_0's l2: 0.0820735\n",
      "[42]\tvalid_0's rmse: 0.286134\tvalid_0's l2: 0.0818726\n",
      "[43]\tvalid_0's rmse: 0.286446\tvalid_0's l2: 0.0820511\n",
      "[44]\tvalid_0's rmse: 0.286845\tvalid_0's l2: 0.08228\n",
      "[45]\tvalid_0's rmse: 0.287275\tvalid_0's l2: 0.0825272\n",
      "[46]\tvalid_0's rmse: 0.286316\tvalid_0's l2: 0.081977\n",
      "[47]\tvalid_0's rmse: 0.286595\tvalid_0's l2: 0.082137\n",
      "[48]\tvalid_0's rmse: 0.286831\tvalid_0's l2: 0.0822719\n",
      "[49]\tvalid_0's rmse: 0.287701\tvalid_0's l2: 0.0827717\n",
      "[50]\tvalid_0's rmse: 0.288403\tvalid_0's l2: 0.0831763\n",
      "[51]\tvalid_0's rmse: 0.288652\tvalid_0's l2: 0.0833198\n",
      "[52]\tvalid_0's rmse: 0.288507\tvalid_0's l2: 0.0832361\n",
      "[53]\tvalid_0's rmse: 0.288787\tvalid_0's l2: 0.0833981\n",
      "[54]\tvalid_0's rmse: 0.288801\tvalid_0's l2: 0.0834059\n",
      "[55]\tvalid_0's rmse: 0.288905\tvalid_0's l2: 0.0834661\n",
      "[56]\tvalid_0's rmse: 0.288087\tvalid_0's l2: 0.0829942\n",
      "[57]\tvalid_0's rmse: 0.288551\tvalid_0's l2: 0.0832619\n",
      "[58]\tvalid_0's rmse: 0.288412\tvalid_0's l2: 0.0831816\n",
      "[59]\tvalid_0's rmse: 0.288556\tvalid_0's l2: 0.0832644\n",
      "[60]\tvalid_0's rmse: 0.288648\tvalid_0's l2: 0.0833179\n",
      "[61]\tvalid_0's rmse: 0.28875\tvalid_0's l2: 0.0833768\n",
      "[62]\tvalid_0's rmse: 0.289119\tvalid_0's l2: 0.0835898\n",
      "[63]\tvalid_0's rmse: 0.289122\tvalid_0's l2: 0.0835915\n",
      "[64]\tvalid_0's rmse: 0.289415\tvalid_0's l2: 0.0837609\n",
      "[65]\tvalid_0's rmse: 0.289308\tvalid_0's l2: 0.0836989\n",
      "[66]\tvalid_0's rmse: 0.289893\tvalid_0's l2: 0.0840378\n",
      "[67]\tvalid_0's rmse: 0.290126\tvalid_0's l2: 0.0841731\n",
      "[68]\tvalid_0's rmse: 0.290583\tvalid_0's l2: 0.0844386\n",
      "[69]\tvalid_0's rmse: 0.29029\tvalid_0's l2: 0.084268\n",
      "[70]\tvalid_0's rmse: 0.290387\tvalid_0's l2: 0.0843248\n",
      "[71]\tvalid_0's rmse: 0.290797\tvalid_0's l2: 0.0845626\n",
      "[72]\tvalid_0's rmse: 0.291377\tvalid_0's l2: 0.0849008\n",
      "[73]\tvalid_0's rmse: 0.291352\tvalid_0's l2: 0.0848862\n",
      "[74]\tvalid_0's rmse: 0.291756\tvalid_0's l2: 0.0851215\n",
      "[75]\tvalid_0's rmse: 0.292334\tvalid_0's l2: 0.0854593\n",
      "[76]\tvalid_0's rmse: 0.292555\tvalid_0's l2: 0.0855884\n",
      "[77]\tvalid_0's rmse: 0.292705\tvalid_0's l2: 0.0856765\n",
      "[78]\tvalid_0's rmse: 0.292934\tvalid_0's l2: 0.0858101\n",
      "[79]\tvalid_0's rmse: 0.292881\tvalid_0's l2: 0.0857794\n",
      "[80]\tvalid_0's rmse: 0.292829\tvalid_0's l2: 0.0857487\n",
      "[81]\tvalid_0's rmse: 0.292958\tvalid_0's l2: 0.0858242\n",
      "[82]\tvalid_0's rmse: 0.293019\tvalid_0's l2: 0.0858601\n",
      "[83]\tvalid_0's rmse: 0.292391\tvalid_0's l2: 0.0854924\n",
      "[84]\tvalid_0's rmse: 0.292906\tvalid_0's l2: 0.0857938\n",
      "[85]\tvalid_0's rmse: 0.29243\tvalid_0's l2: 0.0855151\n",
      "[86]\tvalid_0's rmse: 0.292636\tvalid_0's l2: 0.0856361\n",
      "[87]\tvalid_0's rmse: 0.292367\tvalid_0's l2: 0.0854782\n",
      "[88]\tvalid_0's rmse: 0.292474\tvalid_0's l2: 0.085541\n",
      "[89]\tvalid_0's rmse: 0.292467\tvalid_0's l2: 0.0855369\n",
      "[90]\tvalid_0's rmse: 0.293259\tvalid_0's l2: 0.0860006\n",
      "[91]\tvalid_0's rmse: 0.293346\tvalid_0's l2: 0.0860521\n",
      "[92]\tvalid_0's rmse: 0.293812\tvalid_0's l2: 0.0863256\n",
      "[93]\tvalid_0's rmse: 0.293752\tvalid_0's l2: 0.0862905\n",
      "[94]\tvalid_0's rmse: 0.294031\tvalid_0's l2: 0.0864543\n",
      "[95]\tvalid_0's rmse: 0.294248\tvalid_0's l2: 0.0865817\n",
      "[96]\tvalid_0's rmse: 0.294243\tvalid_0's l2: 0.0865791\n",
      "[97]\tvalid_0's rmse: 0.294341\tvalid_0's l2: 0.0866367\n",
      "[98]\tvalid_0's rmse: 0.294241\tvalid_0's l2: 0.086578\n",
      "[99]\tvalid_0's rmse: 0.294554\tvalid_0's l2: 0.0867622\n",
      "[100]\tvalid_0's rmse: 0.294708\tvalid_0's l2: 0.0868529\n",
      "[101]\tvalid_0's rmse: 0.294836\tvalid_0's l2: 0.0869281\n",
      "[102]\tvalid_0's rmse: 0.294955\tvalid_0's l2: 0.0869987\n",
      "[103]\tvalid_0's rmse: 0.295207\tvalid_0's l2: 0.0871474\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's rmse: 0.275009\tvalid_0's l2: 0.0756297\n",
      "[LightGBM] [Warning] Unknown parameter: sumbsample\n",
      "[1]\tvalid_0's rmse: 0.318068\tvalid_0's l2: 0.101167\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's rmse: 0.315978\tvalid_0's l2: 0.0998423\n",
      "[3]\tvalid_0's rmse: 0.313661\tvalid_0's l2: 0.0983831\n",
      "[4]\tvalid_0's rmse: 0.314749\tvalid_0's l2: 0.0990667\n",
      "[5]\tvalid_0's rmse: 0.314074\tvalid_0's l2: 0.0986423\n",
      "[6]\tvalid_0's rmse: 0.312624\tvalid_0's l2: 0.0977336\n",
      "[7]\tvalid_0's rmse: 0.311611\tvalid_0's l2: 0.0971012\n",
      "[8]\tvalid_0's rmse: 0.309241\tvalid_0's l2: 0.0956299\n",
      "[9]\tvalid_0's rmse: 0.308403\tvalid_0's l2: 0.0951126\n",
      "[10]\tvalid_0's rmse: 0.308046\tvalid_0's l2: 0.0948923\n",
      "[11]\tvalid_0's rmse: 0.3076\tvalid_0's l2: 0.0946178\n",
      "[12]\tvalid_0's rmse: 0.306892\tvalid_0's l2: 0.094183\n",
      "[13]\tvalid_0's rmse: 0.308112\tvalid_0's l2: 0.0949331\n",
      "[14]\tvalid_0's rmse: 0.308863\tvalid_0's l2: 0.0953967\n",
      "[15]\tvalid_0's rmse: 0.310518\tvalid_0's l2: 0.0964213\n",
      "[16]\tvalid_0's rmse: 0.309976\tvalid_0's l2: 0.0960852\n",
      "[17]\tvalid_0's rmse: 0.310398\tvalid_0's l2: 0.0963466\n",
      "[18]\tvalid_0's rmse: 0.31162\tvalid_0's l2: 0.0971071\n",
      "[19]\tvalid_0's rmse: 0.312137\tvalid_0's l2: 0.0974296\n",
      "[20]\tvalid_0's rmse: 0.311774\tvalid_0's l2: 0.0972032\n",
      "[21]\tvalid_0's rmse: 0.311873\tvalid_0's l2: 0.0972646\n",
      "[22]\tvalid_0's rmse: 0.31273\tvalid_0's l2: 0.0978001\n",
      "[23]\tvalid_0's rmse: 0.311892\tvalid_0's l2: 0.0972767\n",
      "[24]\tvalid_0's rmse: 0.312796\tvalid_0's l2: 0.0978416\n",
      "[25]\tvalid_0's rmse: 0.313089\tvalid_0's l2: 0.0980247\n",
      "[26]\tvalid_0's rmse: 0.313147\tvalid_0's l2: 0.098061\n",
      "[27]\tvalid_0's rmse: 0.312838\tvalid_0's l2: 0.0978677\n",
      "[28]\tvalid_0's rmse: 0.312737\tvalid_0's l2: 0.0978046\n",
      "[29]\tvalid_0's rmse: 0.311989\tvalid_0's l2: 0.0973371\n",
      "[30]\tvalid_0's rmse: 0.311598\tvalid_0's l2: 0.0970932\n",
      "[31]\tvalid_0's rmse: 0.312292\tvalid_0's l2: 0.0975265\n",
      "[32]\tvalid_0's rmse: 0.312847\tvalid_0's l2: 0.0978731\n",
      "[33]\tvalid_0's rmse: 0.313236\tvalid_0's l2: 0.0981166\n",
      "[34]\tvalid_0's rmse: 0.314905\tvalid_0's l2: 0.0991651\n",
      "[35]\tvalid_0's rmse: 0.315086\tvalid_0's l2: 0.0992789\n",
      "[36]\tvalid_0's rmse: 0.31609\tvalid_0's l2: 0.0999129\n",
      "[37]\tvalid_0's rmse: 0.317184\tvalid_0's l2: 0.100605\n",
      "[38]\tvalid_0's rmse: 0.318292\tvalid_0's l2: 0.10131\n",
      "[39]\tvalid_0's rmse: 0.318399\tvalid_0's l2: 0.101378\n",
      "[40]\tvalid_0's rmse: 0.319247\tvalid_0's l2: 0.101919\n",
      "[41]\tvalid_0's rmse: 0.320111\tvalid_0's l2: 0.102471\n",
      "[42]\tvalid_0's rmse: 0.321121\tvalid_0's l2: 0.103119\n",
      "[43]\tvalid_0's rmse: 0.322335\tvalid_0's l2: 0.1039\n",
      "[44]\tvalid_0's rmse: 0.322378\tvalid_0's l2: 0.103928\n",
      "[45]\tvalid_0's rmse: 0.322787\tvalid_0's l2: 0.104191\n",
      "[46]\tvalid_0's rmse: 0.322811\tvalid_0's l2: 0.104207\n",
      "[47]\tvalid_0's rmse: 0.323493\tvalid_0's l2: 0.104648\n",
      "[48]\tvalid_0's rmse: 0.324328\tvalid_0's l2: 0.105189\n",
      "[49]\tvalid_0's rmse: 0.323863\tvalid_0's l2: 0.104887\n",
      "[50]\tvalid_0's rmse: 0.323793\tvalid_0's l2: 0.104842\n",
      "[51]\tvalid_0's rmse: 0.324139\tvalid_0's l2: 0.105066\n",
      "[52]\tvalid_0's rmse: 0.324206\tvalid_0's l2: 0.105109\n",
      "[53]\tvalid_0's rmse: 0.324397\tvalid_0's l2: 0.105233\n",
      "[54]\tvalid_0's rmse: 0.325324\tvalid_0's l2: 0.105836\n",
      "[55]\tvalid_0's rmse: 0.325332\tvalid_0's l2: 0.105841\n",
      "[56]\tvalid_0's rmse: 0.325481\tvalid_0's l2: 0.105938\n",
      "[57]\tvalid_0's rmse: 0.325008\tvalid_0's l2: 0.10563\n",
      "[58]\tvalid_0's rmse: 0.324984\tvalid_0's l2: 0.105615\n",
      "[59]\tvalid_0's rmse: 0.325835\tvalid_0's l2: 0.106169\n",
      "[60]\tvalid_0's rmse: 0.326194\tvalid_0's l2: 0.106402\n",
      "[61]\tvalid_0's rmse: 0.326418\tvalid_0's l2: 0.106548\n",
      "[62]\tvalid_0's rmse: 0.32703\tvalid_0's l2: 0.106949\n",
      "[63]\tvalid_0's rmse: 0.328381\tvalid_0's l2: 0.107834\n",
      "[64]\tvalid_0's rmse: 0.328422\tvalid_0's l2: 0.107861\n",
      "[65]\tvalid_0's rmse: 0.328717\tvalid_0's l2: 0.108055\n",
      "[66]\tvalid_0's rmse: 0.329115\tvalid_0's l2: 0.108317\n",
      "[67]\tvalid_0's rmse: 0.329081\tvalid_0's l2: 0.108294\n",
      "[68]\tvalid_0's rmse: 0.329451\tvalid_0's l2: 0.108538\n",
      "[69]\tvalid_0's rmse: 0.329647\tvalid_0's l2: 0.108667\n",
      "[70]\tvalid_0's rmse: 0.329541\tvalid_0's l2: 0.108597\n",
      "[71]\tvalid_0's rmse: 0.329577\tvalid_0's l2: 0.108621\n",
      "[72]\tvalid_0's rmse: 0.330569\tvalid_0's l2: 0.109276\n",
      "[73]\tvalid_0's rmse: 0.331343\tvalid_0's l2: 0.109788\n",
      "[74]\tvalid_0's rmse: 0.331765\tvalid_0's l2: 0.110068\n",
      "[75]\tvalid_0's rmse: 0.332163\tvalid_0's l2: 0.110332\n",
      "[76]\tvalid_0's rmse: 0.332812\tvalid_0's l2: 0.110764\n",
      "[77]\tvalid_0's rmse: 0.332933\tvalid_0's l2: 0.110844\n",
      "[78]\tvalid_0's rmse: 0.333489\tvalid_0's l2: 0.111215\n",
      "[79]\tvalid_0's rmse: 0.333137\tvalid_0's l2: 0.11098\n",
      "[80]\tvalid_0's rmse: 0.333057\tvalid_0's l2: 0.110927\n",
      "[81]\tvalid_0's rmse: 0.333614\tvalid_0's l2: 0.111298\n",
      "[82]\tvalid_0's rmse: 0.333478\tvalid_0's l2: 0.111208\n",
      "[83]\tvalid_0's rmse: 0.3338\tvalid_0's l2: 0.111422\n",
      "[84]\tvalid_0's rmse: 0.333931\tvalid_0's l2: 0.11151\n",
      "[85]\tvalid_0's rmse: 0.334639\tvalid_0's l2: 0.111983\n",
      "[86]\tvalid_0's rmse: 0.334504\tvalid_0's l2: 0.111893\n",
      "[87]\tvalid_0's rmse: 0.335532\tvalid_0's l2: 0.112582\n",
      "[88]\tvalid_0's rmse: 0.335355\tvalid_0's l2: 0.112463\n",
      "[89]\tvalid_0's rmse: 0.336779\tvalid_0's l2: 0.11342\n",
      "[90]\tvalid_0's rmse: 0.336756\tvalid_0's l2: 0.113404\n",
      "[91]\tvalid_0's rmse: 0.337367\tvalid_0's l2: 0.113817\n",
      "[92]\tvalid_0's rmse: 0.337322\tvalid_0's l2: 0.113786\n",
      "[93]\tvalid_0's rmse: 0.337409\tvalid_0's l2: 0.113845\n",
      "[94]\tvalid_0's rmse: 0.338027\tvalid_0's l2: 0.114262\n",
      "[95]\tvalid_0's rmse: 0.338485\tvalid_0's l2: 0.114572\n",
      "[96]\tvalid_0's rmse: 0.33886\tvalid_0's l2: 0.114826\n",
      "[97]\tvalid_0's rmse: 0.33852\tvalid_0's l2: 0.114596\n",
      "[98]\tvalid_0's rmse: 0.338822\tvalid_0's l2: 0.1148\n",
      "[99]\tvalid_0's rmse: 0.338876\tvalid_0's l2: 0.114837\n",
      "[100]\tvalid_0's rmse: 0.339233\tvalid_0's l2: 0.115079\n",
      "[101]\tvalid_0's rmse: 0.339364\tvalid_0's l2: 0.115168\n",
      "[102]\tvalid_0's rmse: 0.340235\tvalid_0's l2: 0.11576\n",
      "[103]\tvalid_0's rmse: 0.340139\tvalid_0's l2: 0.115694\n",
      "[104]\tvalid_0's rmse: 0.339807\tvalid_0's l2: 0.115469\n",
      "[105]\tvalid_0's rmse: 0.340001\tvalid_0's l2: 0.115601\n",
      "[106]\tvalid_0's rmse: 0.340149\tvalid_0's l2: 0.115701\n",
      "[107]\tvalid_0's rmse: 0.340445\tvalid_0's l2: 0.115903\n",
      "[108]\tvalid_0's rmse: 0.340706\tvalid_0's l2: 0.11608\n",
      "[109]\tvalid_0's rmse: 0.340813\tvalid_0's l2: 0.116154\n",
      "[110]\tvalid_0's rmse: 0.341163\tvalid_0's l2: 0.116392\n",
      "[111]\tvalid_0's rmse: 0.341087\tvalid_0's l2: 0.116341\n",
      "[112]\tvalid_0's rmse: 0.340998\tvalid_0's l2: 0.116279\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's rmse: 0.306892\tvalid_0's l2: 0.094183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: sumbsample\n",
      "[1]\tvalid_0's rmse: 0.28149\tvalid_0's l2: 0.0792365\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's rmse: 0.280571\tvalid_0's l2: 0.0787203\n",
      "[3]\tvalid_0's rmse: 0.28239\tvalid_0's l2: 0.079744\n",
      "[4]\tvalid_0's rmse: 0.281892\tvalid_0's l2: 0.0794632\n",
      "[5]\tvalid_0's rmse: 0.283288\tvalid_0's l2: 0.0802523\n",
      "[6]\tvalid_0's rmse: 0.282565\tvalid_0's l2: 0.0798428\n",
      "[7]\tvalid_0's rmse: 0.28339\tvalid_0's l2: 0.0803099\n",
      "[8]\tvalid_0's rmse: 0.284084\tvalid_0's l2: 0.0807038\n",
      "[9]\tvalid_0's rmse: 0.284413\tvalid_0's l2: 0.0808908\n",
      "[10]\tvalid_0's rmse: 0.285602\tvalid_0's l2: 0.0815687\n",
      "[11]\tvalid_0's rmse: 0.286822\tvalid_0's l2: 0.0822671\n",
      "[12]\tvalid_0's rmse: 0.287279\tvalid_0's l2: 0.0825292\n",
      "[13]\tvalid_0's rmse: 0.287537\tvalid_0's l2: 0.0826774\n",
      "[14]\tvalid_0's rmse: 0.288843\tvalid_0's l2: 0.0834304\n",
      "[15]\tvalid_0's rmse: 0.288738\tvalid_0's l2: 0.0833694\n",
      "[16]\tvalid_0's rmse: 0.288708\tvalid_0's l2: 0.0833521\n",
      "[17]\tvalid_0's rmse: 0.288976\tvalid_0's l2: 0.0835069\n",
      "[18]\tvalid_0's rmse: 0.289608\tvalid_0's l2: 0.083873\n",
      "[19]\tvalid_0's rmse: 0.289647\tvalid_0's l2: 0.0838955\n",
      "[20]\tvalid_0's rmse: 0.289607\tvalid_0's l2: 0.0838723\n",
      "[21]\tvalid_0's rmse: 0.290359\tvalid_0's l2: 0.0843086\n",
      "[22]\tvalid_0's rmse: 0.29003\tvalid_0's l2: 0.0841171\n",
      "[23]\tvalid_0's rmse: 0.288467\tvalid_0's l2: 0.083213\n",
      "[24]\tvalid_0's rmse: 0.288473\tvalid_0's l2: 0.0832166\n",
      "[25]\tvalid_0's rmse: 0.288336\tvalid_0's l2: 0.0831377\n",
      "[26]\tvalid_0's rmse: 0.288751\tvalid_0's l2: 0.0833772\n",
      "[27]\tvalid_0's rmse: 0.288965\tvalid_0's l2: 0.0835007\n",
      "[28]\tvalid_0's rmse: 0.288993\tvalid_0's l2: 0.0835172\n",
      "[29]\tvalid_0's rmse: 0.288351\tvalid_0's l2: 0.0831465\n",
      "[30]\tvalid_0's rmse: 0.288502\tvalid_0's l2: 0.0832336\n",
      "[31]\tvalid_0's rmse: 0.289814\tvalid_0's l2: 0.0839924\n",
      "[32]\tvalid_0's rmse: 0.290092\tvalid_0's l2: 0.0841531\n",
      "[33]\tvalid_0's rmse: 0.290898\tvalid_0's l2: 0.0846215\n",
      "[34]\tvalid_0's rmse: 0.290889\tvalid_0's l2: 0.0846163\n",
      "[35]\tvalid_0's rmse: 0.290604\tvalid_0's l2: 0.0844505\n",
      "[36]\tvalid_0's rmse: 0.290871\tvalid_0's l2: 0.0846061\n",
      "[37]\tvalid_0's rmse: 0.290586\tvalid_0's l2: 0.0844401\n",
      "[38]\tvalid_0's rmse: 0.290203\tvalid_0's l2: 0.0842179\n",
      "[39]\tvalid_0's rmse: 0.290125\tvalid_0's l2: 0.0841723\n",
      "[40]\tvalid_0's rmse: 0.289988\tvalid_0's l2: 0.084093\n",
      "[41]\tvalid_0's rmse: 0.290395\tvalid_0's l2: 0.0843292\n",
      "[42]\tvalid_0's rmse: 0.290151\tvalid_0's l2: 0.0841875\n",
      "[43]\tvalid_0's rmse: 0.289855\tvalid_0's l2: 0.084016\n",
      "[44]\tvalid_0's rmse: 0.289851\tvalid_0's l2: 0.0840134\n",
      "[45]\tvalid_0's rmse: 0.290117\tvalid_0's l2: 0.0841681\n",
      "[46]\tvalid_0's rmse: 0.289689\tvalid_0's l2: 0.0839199\n",
      "[47]\tvalid_0's rmse: 0.289288\tvalid_0's l2: 0.0836876\n",
      "[48]\tvalid_0's rmse: 0.289851\tvalid_0's l2: 0.0840137\n",
      "[49]\tvalid_0's rmse: 0.29006\tvalid_0's l2: 0.084135\n",
      "[50]\tvalid_0's rmse: 0.290389\tvalid_0's l2: 0.0843257\n",
      "[51]\tvalid_0's rmse: 0.291126\tvalid_0's l2: 0.0847542\n",
      "[52]\tvalid_0's rmse: 0.290452\tvalid_0's l2: 0.0843624\n",
      "[53]\tvalid_0's rmse: 0.290534\tvalid_0's l2: 0.0844097\n",
      "[54]\tvalid_0's rmse: 0.290246\tvalid_0's l2: 0.0842426\n",
      "[55]\tvalid_0's rmse: 0.290347\tvalid_0's l2: 0.0843015\n",
      "[56]\tvalid_0's rmse: 0.290725\tvalid_0's l2: 0.0845213\n",
      "[57]\tvalid_0's rmse: 0.291192\tvalid_0's l2: 0.0847929\n",
      "[58]\tvalid_0's rmse: 0.291898\tvalid_0's l2: 0.0852046\n",
      "[59]\tvalid_0's rmse: 0.291771\tvalid_0's l2: 0.0851303\n",
      "[60]\tvalid_0's rmse: 0.292345\tvalid_0's l2: 0.0854658\n",
      "[61]\tvalid_0's rmse: 0.292727\tvalid_0's l2: 0.085689\n",
      "[62]\tvalid_0's rmse: 0.292537\tvalid_0's l2: 0.0855781\n",
      "[63]\tvalid_0's rmse: 0.292579\tvalid_0's l2: 0.0856025\n",
      "[64]\tvalid_0's rmse: 0.292341\tvalid_0's l2: 0.085463\n",
      "[65]\tvalid_0's rmse: 0.292609\tvalid_0's l2: 0.0856202\n",
      "[66]\tvalid_0's rmse: 0.293379\tvalid_0's l2: 0.086071\n",
      "[67]\tvalid_0's rmse: 0.293565\tvalid_0's l2: 0.0861806\n",
      "[68]\tvalid_0's rmse: 0.294149\tvalid_0's l2: 0.0865236\n",
      "[69]\tvalid_0's rmse: 0.294646\tvalid_0's l2: 0.0868165\n",
      "[70]\tvalid_0's rmse: 0.294628\tvalid_0's l2: 0.0868059\n",
      "[71]\tvalid_0's rmse: 0.294646\tvalid_0's l2: 0.0868163\n",
      "[72]\tvalid_0's rmse: 0.294694\tvalid_0's l2: 0.0868447\n",
      "[73]\tvalid_0's rmse: 0.294799\tvalid_0's l2: 0.0869064\n",
      "[74]\tvalid_0's rmse: 0.294594\tvalid_0's l2: 0.0867857\n",
      "[75]\tvalid_0's rmse: 0.294993\tvalid_0's l2: 0.0870208\n",
      "[76]\tvalid_0's rmse: 0.295191\tvalid_0's l2: 0.0871377\n",
      "[77]\tvalid_0's rmse: 0.29524\tvalid_0's l2: 0.0871664\n",
      "[78]\tvalid_0's rmse: 0.295568\tvalid_0's l2: 0.0873605\n",
      "[79]\tvalid_0's rmse: 0.295621\tvalid_0's l2: 0.0873919\n",
      "[80]\tvalid_0's rmse: 0.295564\tvalid_0's l2: 0.0873579\n",
      "[81]\tvalid_0's rmse: 0.295637\tvalid_0's l2: 0.0874012\n",
      "[82]\tvalid_0's rmse: 0.295565\tvalid_0's l2: 0.0873585\n",
      "[83]\tvalid_0's rmse: 0.295538\tvalid_0's l2: 0.0873428\n",
      "[84]\tvalid_0's rmse: 0.29572\tvalid_0's l2: 0.0874503\n",
      "[85]\tvalid_0's rmse: 0.29592\tvalid_0's l2: 0.0875686\n",
      "[86]\tvalid_0's rmse: 0.295966\tvalid_0's l2: 0.087596\n",
      "[87]\tvalid_0's rmse: 0.296282\tvalid_0's l2: 0.0877827\n",
      "[88]\tvalid_0's rmse: 0.295842\tvalid_0's l2: 0.0875226\n",
      "[89]\tvalid_0's rmse: 0.296615\tvalid_0's l2: 0.0879804\n",
      "[90]\tvalid_0's rmse: 0.297111\tvalid_0's l2: 0.0882752\n",
      "[91]\tvalid_0's rmse: 0.298286\tvalid_0's l2: 0.0889744\n",
      "[92]\tvalid_0's rmse: 0.299162\tvalid_0's l2: 0.089498\n",
      "[93]\tvalid_0's rmse: 0.299151\tvalid_0's l2: 0.0894911\n",
      "[94]\tvalid_0's rmse: 0.298468\tvalid_0's l2: 0.0890833\n",
      "[95]\tvalid_0's rmse: 0.298901\tvalid_0's l2: 0.0893417\n",
      "[96]\tvalid_0's rmse: 0.298947\tvalid_0's l2: 0.0893693\n",
      "[97]\tvalid_0's rmse: 0.298773\tvalid_0's l2: 0.0892654\n",
      "[98]\tvalid_0's rmse: 0.299197\tvalid_0's l2: 0.0895188\n",
      "[99]\tvalid_0's rmse: 0.299473\tvalid_0's l2: 0.0896843\n",
      "[100]\tvalid_0's rmse: 0.29963\tvalid_0's l2: 0.0897779\n",
      "[101]\tvalid_0's rmse: 0.300572\tvalid_0's l2: 0.0903438\n",
      "[102]\tvalid_0's rmse: 0.300722\tvalid_0's l2: 0.0904338\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's rmse: 0.280571\tvalid_0's l2: 0.0787203\n"
     ]
    }
   ],
   "source": [
    "fold = KFold(n_splits=5)\n",
    "n_iter=0\n",
    "cv_lgbm = []\n",
    "lgbm_mean=0\n",
    "for train_index, test_index in fold.split(X_data, y_data):\n",
    "    n_iter += 1\n",
    "    X_train= X_data.iloc[train_index]\n",
    "    X_test= X_data.iloc[test_index]\n",
    "    y_train= y_data.iloc[train_index]\n",
    "    y_test= y_data.iloc[test_index]\n",
    "    lgbm = LGBMRegressor(n_estimators=300, learning_rate=0.1565, num_leaves=36, sumbsample=0.8691, colsample_bytree=0.7735, max_depth=10,\n",
    "                             min_child_weight=5.798)\n",
    "    evals = [(X_test, y_test)]\n",
    "    lgbm.fit(X_train, y_train, early_stopping_rounds=100, eval_metric='rmse', eval_set=evals, verbose=True)\n",
    "    preds = lgbm.predict(X_test)\n",
    "    lgbm_rmse=math.sqrt(mean_squared_error(preds, y_test))\n",
    "    cv_lgbm.append(lgbm_rmse)\n",
    "    lgbm_mean+=lgbm_rmse/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bf6ab507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 0.23854770162042407\n",
      "2번째 0.24222196180940048\n",
      "3번째 0.2750085935462259\n",
      "4번째 0.306892446958668\n",
      "5번째 0.2805714063189305\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    rmse=cv_lgbm[i]\n",
    "    print(f'{i+1}번째 {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fa5ac5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2686484220507298"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc87e0",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cd0f7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from catboost import CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8c153a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = 2000\n",
    "seed = 45\n",
    "n_fold = 5 #fold 수\n",
    "\n",
    "X=analysis.iloc[:, :-1]\n",
    "y=analysis.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "801f21c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SP_1</th>\n",
       "      <th>SP_2</th>\n",
       "      <th>SP_3</th>\n",
       "      <th>SP_4</th>\n",
       "      <th>SP_56</th>\n",
       "      <th>ANGLE_1</th>\n",
       "      <th>ANGLE_2</th>\n",
       "      <th>ANGLE_3</th>\n",
       "      <th>ANGLE_4</th>\n",
       "      <th>ANGLE_5</th>\n",
       "      <th>ANGLE_6</th>\n",
       "      <th>L1_S1</th>\n",
       "      <th>L1_S2</th>\n",
       "      <th>L2_S1</th>\n",
       "      <th>L2_S2</th>\n",
       "      <th>L3_S1</th>\n",
       "      <th>L3_S2</th>\n",
       "      <th>L4_S1</th>\n",
       "      <th>L4_S2</th>\n",
       "      <th>L5_S1</th>\n",
       "      <th>L5_S2</th>\n",
       "      <th>L6_S1</th>\n",
       "      <th>L6_S2</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.442515</td>\n",
       "      <td>0.525762</td>\n",
       "      <td>0.053614</td>\n",
       "      <td>0.168752</td>\n",
       "      <td>0.019468</td>\n",
       "      <td>0.148479</td>\n",
       "      <td>0.205481</td>\n",
       "      <td>0.017360</td>\n",
       "      <td>0.155237</td>\n",
       "      <td>0.713853</td>\n",
       "      <td>0.240824</td>\n",
       "      <td>0.155214</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.442700</td>\n",
       "      <td>0.525710</td>\n",
       "      <td>0.054203</td>\n",
       "      <td>0.168264</td>\n",
       "      <td>0.019107</td>\n",
       "      <td>0.149044</td>\n",
       "      <td>0.205642</td>\n",
       "      <td>0.017524</td>\n",
       "      <td>0.155110</td>\n",
       "      <td>0.713473</td>\n",
       "      <td>0.240843</td>\n",
       "      <td>0.155461</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.442282</td>\n",
       "      <td>0.525652</td>\n",
       "      <td>0.053630</td>\n",
       "      <td>0.168604</td>\n",
       "      <td>0.019629</td>\n",
       "      <td>0.148837</td>\n",
       "      <td>0.205574</td>\n",
       "      <td>0.017938</td>\n",
       "      <td>0.155033</td>\n",
       "      <td>0.713326</td>\n",
       "      <td>0.241038</td>\n",
       "      <td>0.155102</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.442561</td>\n",
       "      <td>0.525580</td>\n",
       "      <td>0.053746</td>\n",
       "      <td>0.168763</td>\n",
       "      <td>0.019377</td>\n",
       "      <td>0.147993</td>\n",
       "      <td>0.205102</td>\n",
       "      <td>0.017605</td>\n",
       "      <td>0.155838</td>\n",
       "      <td>0.714428</td>\n",
       "      <td>0.240919</td>\n",
       "      <td>0.154958</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.442795</td>\n",
       "      <td>0.525435</td>\n",
       "      <td>0.054720</td>\n",
       "      <td>0.169310</td>\n",
       "      <td>0.020319</td>\n",
       "      <td>0.148719</td>\n",
       "      <td>0.205894</td>\n",
       "      <td>0.017623</td>\n",
       "      <td>0.155668</td>\n",
       "      <td>0.713284</td>\n",
       "      <td>0.241035</td>\n",
       "      <td>0.155063</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.442700</td>\n",
       "      <td>0.525710</td>\n",
       "      <td>0.054882</td>\n",
       "      <td>0.168189</td>\n",
       "      <td>0.019107</td>\n",
       "      <td>0.149044</td>\n",
       "      <td>0.205642</td>\n",
       "      <td>0.017524</td>\n",
       "      <td>0.155110</td>\n",
       "      <td>0.713473</td>\n",
       "      <td>0.240843</td>\n",
       "      <td>0.155461</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.442700</td>\n",
       "      <td>0.525710</td>\n",
       "      <td>0.054882</td>\n",
       "      <td>0.168189</td>\n",
       "      <td>0.019107</td>\n",
       "      <td>0.149044</td>\n",
       "      <td>0.205642</td>\n",
       "      <td>0.017524</td>\n",
       "      <td>0.155110</td>\n",
       "      <td>0.713473</td>\n",
       "      <td>0.240843</td>\n",
       "      <td>0.155461</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>26.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.442800</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.054530</td>\n",
       "      <td>0.168647</td>\n",
       "      <td>0.019036</td>\n",
       "      <td>0.147084</td>\n",
       "      <td>0.205212</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>0.154558</td>\n",
       "      <td>0.712324</td>\n",
       "      <td>0.240869</td>\n",
       "      <td>0.154827</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>28.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.442713</td>\n",
       "      <td>0.525314</td>\n",
       "      <td>0.054732</td>\n",
       "      <td>0.169486</td>\n",
       "      <td>0.020727</td>\n",
       "      <td>0.149014</td>\n",
       "      <td>0.205266</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.156186</td>\n",
       "      <td>0.712164</td>\n",
       "      <td>0.240643</td>\n",
       "      <td>0.154965</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.442713</td>\n",
       "      <td>0.525314</td>\n",
       "      <td>0.054732</td>\n",
       "      <td>0.169486</td>\n",
       "      <td>0.020727</td>\n",
       "      <td>0.149014</td>\n",
       "      <td>0.205266</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.156186</td>\n",
       "      <td>0.712164</td>\n",
       "      <td>0.240643</td>\n",
       "      <td>0.154965</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>656 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SP_1  SP_2  SP_3  SP_4  SP_56  ANGLE_1  ANGLE_2  ANGLE_3  ANGLE_4  \\\n",
       "0    28.0  26.0  28.0  28.0  493.0    180.0    270.0     90.0    270.0   \n",
       "1    28.0  28.0  26.0  24.0  497.0     90.0    270.0      0.0    270.0   \n",
       "2    28.0  28.0  28.0  30.0  493.0    270.0    270.0      0.0    270.0   \n",
       "3    28.0  28.0  30.0  30.0  492.0    225.0    270.0    135.0    315.0   \n",
       "4    28.0  26.0  28.0  30.0  493.0      0.0     45.0      0.0     45.0   \n",
       "..    ...   ...   ...   ...    ...      ...      ...      ...      ...   \n",
       "651  26.0  26.0  28.0  28.0  498.0     90.0    270.0      0.0    225.0   \n",
       "652  26.0  26.0  28.0  30.0  498.0     90.0    270.0      0.0    225.0   \n",
       "653  26.0  24.0  28.0  32.0  494.0    315.0    270.0    225.0    315.0   \n",
       "654  28.0  24.0  28.0  26.0  493.0    180.0    180.0    180.0      0.0   \n",
       "655  28.0  26.0  28.0  26.0  493.0    180.0    180.0    180.0      0.0   \n",
       "\n",
       "     ANGLE_5  ANGLE_6     L1_S1     L1_S2     L2_S1     L2_S2     L3_S1  \\\n",
       "0      270.0    180.0  0.442515  0.525762  0.053614  0.168752  0.019468   \n",
       "1      180.0      0.0  0.442700  0.525710  0.054203  0.168264  0.019107   \n",
       "2      180.0    180.0  0.442282  0.525652  0.053630  0.168604  0.019629   \n",
       "3      315.0    180.0  0.442561  0.525580  0.053746  0.168763  0.019377   \n",
       "4      315.0     90.0  0.442795  0.525435  0.054720  0.169310  0.020319   \n",
       "..       ...      ...       ...       ...       ...       ...       ...   \n",
       "651    180.0      0.0  0.442700  0.525710  0.054882  0.168189  0.019107   \n",
       "652    180.0      0.0  0.442700  0.525710  0.054882  0.168189  0.019107   \n",
       "653    180.0     90.0  0.442800  0.525800  0.054530  0.168647  0.019036   \n",
       "654      0.0     90.0  0.442713  0.525314  0.054732  0.169486  0.020727   \n",
       "655      0.0     90.0  0.442713  0.525314  0.054732  0.169486  0.020727   \n",
       "\n",
       "        L3_S2     L4_S1     L4_S2     L5_S1     L5_S2     L6_S1     L6_S2  \\\n",
       "0    0.148479  0.205481  0.017360  0.155237  0.713853  0.240824  0.155214   \n",
       "1    0.149044  0.205642  0.017524  0.155110  0.713473  0.240843  0.155461   \n",
       "2    0.148837  0.205574  0.017938  0.155033  0.713326  0.241038  0.155102   \n",
       "3    0.147993  0.205102  0.017605  0.155838  0.714428  0.240919  0.154958   \n",
       "4    0.148719  0.205894  0.017623  0.155668  0.713284  0.241035  0.155063   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "651  0.149044  0.205642  0.017524  0.155110  0.713473  0.240843  0.155461   \n",
       "652  0.149044  0.205642  0.017524  0.155110  0.713473  0.240843  0.155461   \n",
       "653  0.147084  0.205212  0.018091  0.154558  0.712324  0.240869  0.154827   \n",
       "654  0.149014  0.205266  0.017699  0.156186  0.712164  0.240643  0.154965   \n",
       "655  0.149014  0.205266  0.017699  0.156186  0.712164  0.240643  0.154965   \n",
       "\n",
       "        F1     F2     F3     F4     F5  \n",
       "0    0.360  0.380  0.384  0.383  0.443  \n",
       "1    0.360  0.380  0.383  0.384  0.443  \n",
       "2    0.360  0.381  0.383  0.383  0.441  \n",
       "3    0.361  0.379  0.382  0.383  0.442  \n",
       "4    0.361  0.381  0.382  0.384  0.442  \n",
       "..     ...    ...    ...    ...    ...  \n",
       "651  0.360  0.380  0.383  0.384  0.443  \n",
       "652  0.360  0.380  0.383  0.384  0.443  \n",
       "653  0.360  0.380  0.385  0.383  0.443  \n",
       "654  0.360  0.380  0.384  0.384  0.442  \n",
       "655  0.360  0.380  0.384  0.384  0.442  \n",
       "\n",
       "[656 rows x 28 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "03fd9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.astype('str')\n",
    "X['L1_S1']=X['L1_S1'].astype(float)\n",
    "X['L1_S2']=X['L1_S2'].astype(float)\n",
    "X['L2_S1']=X['L2_S1'].astype(float)\n",
    "X['L2_S2']=X['L2_S2'].astype(float)\n",
    "X['L3_S1']=X['L3_S1'].astype(float)\n",
    "X['L3_S2']=X['L3_S2'].astype(float)\n",
    "X['L4_S1']=X['L4_S1'].astype(float)\n",
    "X['L4_S2']=X['L4_S2'].astype(float)\n",
    "X['L5_S1']=X['L5_S1'].astype(float)\n",
    "X['L5_S2']=X['L5_S2'].astype(float)\n",
    "X['L6_S1']=X['L6_S1'].astype(float)\n",
    "X['L6_S2']=X['L6_S2'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c8f3621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = X.dtypes[X.dtypes == \"object\"].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "711c9ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Fold 0 -----------------\n",
      "\n",
      "Learning rate set to 0.040779\n",
      "0:\tlearn: 0.2785909\ttest: 0.2659436\tbest: 0.2659436 (0)\ttotal: 124ms\tremaining: 2m 4s\n",
      "100:\tlearn: 0.2383228\ttest: 0.2607668\tbest: 0.2603766 (93)\ttotal: 7.11s\tremaining: 1m 3s\n",
      "200:\tlearn: 0.2170674\ttest: 0.2615957\tbest: 0.2600924 (154)\ttotal: 13.2s\tremaining: 52.4s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.2600924349\n",
      "bestIteration = 154\n",
      "\n",
      "Shrink model to first 155 iterations.\n",
      "\n",
      "----------------- Fold 1 -----------------\n",
      "\n",
      "Learning rate set to 0.040794\n",
      "0:\tlearn: 0.2815083\ttest: 0.2549601\tbest: 0.2549601 (0)\ttotal: 54ms\tremaining: 53.9s\n",
      "100:\tlearn: 0.2478594\ttest: 0.2483354\tbest: 0.2480496 (85)\ttotal: 6.02s\tremaining: 53.6s\n",
      "200:\tlearn: 0.2239834\ttest: 0.2475795\tbest: 0.2469439 (180)\ttotal: 10.8s\tremaining: 42.8s\n",
      "300:\tlearn: 0.2059562\ttest: 0.2480094\tbest: 0.2465167 (245)\ttotal: 16.3s\tremaining: 37.9s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.2465166763\n",
      "bestIteration = 245\n",
      "\n",
      "Shrink model to first 246 iterations.\n",
      "\n",
      "----------------- Fold 2 -----------------\n",
      "\n",
      "Learning rate set to 0.040794\n",
      "0:\tlearn: 0.2779980\ttest: 0.2687178\tbest: 0.2687178 (0)\ttotal: 33.1ms\tremaining: 33.1s\n",
      "100:\tlearn: 0.2375366\ttest: 0.2662549\tbest: 0.2650113 (85)\ttotal: 7.12s\tremaining: 1m 3s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.2650113459\n",
      "bestIteration = 85\n",
      "\n",
      "Shrink model to first 86 iterations.\n",
      "\n",
      "----------------- Fold 3 -----------------\n",
      "\n",
      "Learning rate set to 0.040794\n",
      "0:\tlearn: 0.2691284\ttest: 0.3057609\tbest: 0.3057609 (0)\ttotal: 74.5ms\tremaining: 1m 14s\n",
      "100:\tlearn: 0.2306611\ttest: 0.2903811\tbest: 0.2903811 (100)\ttotal: 5.04s\tremaining: 44.9s\n",
      "200:\tlearn: 0.2059550\ttest: 0.2866633\tbest: 0.2866633 (200)\ttotal: 10.9s\tremaining: 43.3s\n",
      "300:\tlearn: 0.1879554\ttest: 0.2850661\tbest: 0.2850188 (298)\ttotal: 18.1s\tremaining: 42s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.2850187513\n",
      "bestIteration = 298\n",
      "\n",
      "Shrink model to first 299 iterations.\n",
      "\n",
      "----------------- Fold 4 -----------------\n",
      "\n",
      "Learning rate set to 0.040794\n",
      "0:\tlearn: 0.2738253\ttest: 0.2871207\tbest: 0.2871207 (0)\ttotal: 48ms\tremaining: 47.9s\n",
      "100:\tlearn: 0.2389473\ttest: 0.2786179\tbest: 0.2786179 (100)\ttotal: 6.35s\tremaining: 56.5s\n",
      "200:\tlearn: 0.2185986\ttest: 0.2783297\tbest: 0.2777531 (166)\ttotal: 12s\tremaining: 47.5s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.2777530838\n",
      "bestIteration = 166\n",
      "\n",
      "Shrink model to first 167 iterations.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "kfold = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "cv_rmse=[]\n",
    "folds=[]\n",
    "cat_mean=0\n",
    "for train_idx, valid_idx in kfold.split(X, y):\n",
    "        folds.append((train_idx, valid_idx))\n",
    "\n",
    "\n",
    "for fold in range(n_fold):\n",
    "    print(f'\\n----------------- Fold {fold} -----------------\\n')\n",
    "    train_idx, valid_idx = folds[fold]\n",
    "    X_train, X_valid, y_train, y_valid = X.iloc[train_idx], X.iloc[valid_idx], y[train_idx], y[valid_idx]\n",
    "    train_data = Pool(data=X_train, label=y_train, cat_features=lists)\n",
    "    valid_data = Pool(data=X_valid, label=y_valid, cat_features=lists)\n",
    "\n",
    "    model_reg = CatBoostRegressor()\n",
    "    model_reg.fit(train_data, eval_set=valid_data, use_best_model=True, early_stopping_rounds=100, verbose=100)\n",
    "\n",
    "    reg_preds = model_reg.predict(X_valid)\n",
    "    rmse=math.sqrt(mean_squared_error(reg_preds, y_valid))\n",
    "    cv_rmse.append(rmse)\n",
    "    cat_mean+=rmse / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b101175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 0.2600924354824988\n",
      "2번째 0.24651667660414667\n",
      "3번째 0.2650113480244863\n",
      "4번째 0.28501875163265045\n",
      "5번째 0.27775308411936567\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    rmse = cv_rmse[i]\n",
    "    print(f'{i+1}번째 {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "257d8b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2668784591726296"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "af54a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "# alphas list 값을 반복하면서 alpha에 따른 평균 rmse를 구함\n",
    "# cross_val_score를 이용해 5 폴드의 평균 RMSE를 계산\n",
    "# alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀계수 값들을 DataFrame으로 반환\n",
    "y_target = analysis['yield']\n",
    "X_data = analysis.drop(['yield'], axis=1, inplace=False)\n",
    "\n",
    "def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True):\n",
    "    coeff_df = pd.DataFrame()\n",
    "    if verbose : print('######', model_name, '######')\n",
    "    for param in params:\n",
    "        if model_name =='Ridge': model = Ridge(alpha=param)\n",
    "        elif model_name =='Lasso' : model = Lasso(alpha=param)\n",
    "        elif model_name =='ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)\n",
    "        neg_mse_scores = cross_val_score(model, X_data_n, y_target_n,\n",
    "                                        scoring='neg_mean_squared_error', cv = 5)\n",
    "        avg_rmse = np.mean(np.sqrt(-1*neg_mse_scores))\n",
    "        print('alpha {0}일 때 5 폴드 세트의 평균 RMSE: {1:.3f}'.format(param, avg_rmse))\n",
    "        model.fit(X_data, y_target)\n",
    "        \n",
    "        coeff = pd.Series(data=model.coef_, index=X_data.columns )\n",
    "        colname='alpha:'+str(param)\n",
    "        coeff_df[colname] = coeff\n",
    "    return coeff_df\n",
    "\n",
    "alphas = [0.07, 0.1, 0.5, 1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2ccb21f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Ridge ######\n",
      "alpha 0.07일 때 5 폴드 세트의 평균 RMSE: 0.283\n",
      "alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 0.282\n",
      "alpha 0.5일 때 5 폴드 세트의 평균 RMSE: 0.282\n",
      "alpha 1일 때 5 폴드 세트의 평균 RMSE: 0.282\n",
      "alpha 3일 때 5 폴드 세트의 평균 RMSE: 0.282\n"
     ]
    }
   ],
   "source": [
    "# 릿지 회귀\n",
    "coeff_ridge_df = get_linear_reg_eval('Ridge', params=alphas, X_data_n=X_data, y_target_n=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "73c7f640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Lasso ######\n",
      "alpha 0.07일 때 5 폴드 세트의 평균 RMSE: 0.278\n",
      "alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 0.278\n",
      "alpha 0.5일 때 5 폴드 세트의 평균 RMSE: 0.277\n",
      "alpha 1일 때 5 폴드 세트의 평균 RMSE: 0.276\n",
      "alpha 3일 때 5 폴드 세트의 평균 RMSE: 0.276\n"
     ]
    }
   ],
   "source": [
    "# 라쏘 회귀\n",
    "coeff_ridge_df = get_linear_reg_eval('Lasso', params=alphas, X_data_n=X_data, y_target_n=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "90d4b260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### ElasticNet ######\n",
      "alpha 0.07일 때 5 폴드 세트의 평균 RMSE: 0.278\n",
      "alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 0.278\n",
      "alpha 0.5일 때 5 폴드 세트의 평균 RMSE: 0.277\n",
      "alpha 1일 때 5 폴드 세트의 평균 RMSE: 0.277\n",
      "alpha 3일 때 5 폴드 세트의 평균 RMSE: 0.276\n"
     ]
    }
   ],
   "source": [
    "### 엘라스틱넷 회귀는 L2규제와 L1 규제를 결합한 회귀 \n",
    "coeff_elastic_df = get_linear_reg_eval('ElasticNet', params=alphas,\n",
    "                                      X_data_n=X, y_target_n=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d3626",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8d644c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# 필요한 패키지/모듈 가져오기\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "46ea1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dtree = DecisionTreeRegressor()\n",
    "\n",
    "parameters = {'max_depth':[1,2,3,4], 'min_samples_split':[5,10,15,20]}\n",
    "grid_dtree = GridSearchCV(dtree, param_grid=parameters, scoring='neg_mean_squared_error',cv=5, refit=True)\n",
    "grid_dtree.fit(X_data, y_data)\n",
    "tree_rmse= np.sqrt(-1*grid_dtree.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a2973391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2749975230929241"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287163c",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e5919e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bfcbdf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators':(100, 200),\n",
    "    'max_depth' : (3, 5, 7, 9, 10, 12),\n",
    "    'min_samples_leaf' : (5, 10, 15, 20, 25, 30)\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=0, n_jobs=-1)\n",
    "grid_forest = GridSearchCV(rf, param_grid=params, cv=5, scoring='neg_mean_squared_error', refit=True)\n",
    "grid_forest.fit(X_data, y_data)\n",
    "\n",
    "\n",
    "forest_rmse= np.sqrt(-1*grid_forest.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f6e77d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27113314447530323"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946446ef",
   "metadata": {},
   "source": [
    "## 최종모델 변수중요도(CatBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "220155ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt # 변수 중요도\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "ftr_importances_values = model_reg.feature_importances_\n",
    "ftr_importances = pd.Series(ftr_importances_values, index=X_data.columns)\n",
    "ftr_top = ftr_importances.sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=ftr_top, y=ftr_top.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210bfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
